% SIAM Article Template
\documentclass[review,onefignum,onetabnum]{siamart171218}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

\input{ex_shared}

% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={GreedyOWNS},
  pdfauthor={M.K. Sleeman, and T. Colonius}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

%% Use \myexternaldocument on Overleaf
\myexternaldocument{ex_supplement}

% FundRef data to be entered by SIAM
%<funding-group>
%<award-group>
%<funding-source>
%<named-content content-type="funder-name"> 
%</named-content> 
%<named-content content-type="funder-identifier"> 
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

\usepackage{bm}
\usepackage{subcaption}

% comment
\usepackage{xcolor}
\newcommand{\michael}[1]{\textcolor{magenta}{Michael: #1}}

\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
Solutions to hyperbolic systems comprise waves propagating at finite speeds. When wave propagation is predominantly unidirectional, one-way wave equations can be used to evolve only the right-going solution by removing support for left-going waves. The One-Way Navier-Stokes (OWNS) approach, which was originally developed for systems of first-order hyperbolic equations, constructs one-way approximations to the linearized Navier-Stokes equations using a recursive filter to remove left-going waves. The computational cost scales with the number of recursion parameters, which must be carefully chosen to ensure accuracy and stability of the resulting one-way equation. Previous work has chosen parameters based on heuristic estimates of key eigenvalues, which requires trial-and-error tuning while also yielding slow error convergence. We propose a greedy algorithm for automatic parameter selection, which we show yields faster convergence and a net decrease in computational cost for linear and nonlinear disturbance evolution in boundary-layer flows. We review the OWNS projection (OWNS-P) and recursive (OWNS-R) methods, comparing their convergence properties, and show through our numerical analysis and experiments that OWNS-P yields superior convergence and stability properties. Although we demonstrate the method for Navier-Stokes equations, we perform our analyses on systems of linear first-order hyperbolic equations and emphasize that the greedy algorithm is applicable to such systems.
\end{abstract}

% REQUIRED
\begin{keywords}
  hyperbolic, one-way equation, parabolic approximation, greedy algorithm, spatial marching
\end{keywords}

% REQUIRED
\begin{AMS}
35L50, 65M99, 76E09
\end{AMS}

\section{Introduction}

Wave propagation can be modeled by (linear) systems of first-order hyperbolic partial differential equations (PDEs), solvable either as time-domain initial boundary value problems or frequency-domain boundary value problems. When wave propagation is predominantly unidirectional, one-way wave equations can be used to evolve the right-going solution by removing support for left-going waves. Such equations can be obtained by factoring the dispersion relation in Fourierâ€“Laplace space into left- and right-going factors, and then retaining only the right-going branch. Since the resulting equation contains a square root of the Fourier-Laplace variables, the transformation back to physical space results in a nonlocal integro-differential equation, which can be localized using rational approximations of the square root~\cite{Lee_2000_Parabolic}. Although these methods are accurate, well-posed, and efficient for simple wave equations~\cite{Trefethen_1986_Parabolic,Halpern_1988_OneWay} such as the equations for geophysical migration of seismic waves~\cite{Claerbout_1976_Geophysics,Claerbout_1985_Geophysics} and underwater acoustics~\cite{Collins_1989_Underwater,Jensen_1995_Underwater}, they can only be applied in cases where the eigenvalues can be determined analytically, since the dispersion relation must be factored analytically. Methods for one-way marching that do not depend on this factorization have been developed by Guddati~\cite{Guddati_2006_OneWay} for acoustic and elastic wave equations, and by Towne and Colonius~\cite{Towne_2015_OWNS-O} for general systems of first-order hyperbolic equations.

While the linearized Euler equations comprise a system of first-order hyperbolic equations, the linearized Navier-Stokes do not. Nevertheless, the One-Way Navier-Stokes (OWNS) framework generalizes the approach of Towne and Colonius to the linearized Navier-Stokes equations~\cite{Towne_2022_OWNS-P,Kamal_2020_HOWNS,Kamal_2021_OWNS_IO,Kamal_2022_HOWNS,Rigas_2017_OWNS_BL}, where it has been used for linear disturbance evolution in turbulent jets and boundary layers. When applied to the Navier-Stokes equations, the method is now referred to as the OWNS outflow (OWNS-O) approach since it is based on work by Givoli and Neta~\cite{Givoli_2003_NRBC} and Hagstrom and Warburton~\cite{Hagstrom_2004_NRBC} for non-reflective boundary conditions (NRBCs) at ``outflow'' boundaries. For brevity, we will use the OWNS label, but we will perform our analysis on systems of linear first-order hyperbolic equations, and refer to Towne et al.~\cite{Towne_2022_OWNS-P} for the details on generalizing the method to the Navier-Stokes equations.

The \textit{exact} formulation of OWNS-O avoids factoring the dispersion relation by (i) discretizing in the transverse directions to obtain a semi-discrete ordinary differential equation (ODE) in the marching direction~\eqref{eq:HyperbolicPDE_D_char_freq}, (ii) computing the eigendecomposition of the discretized linear operator, $M$ in~\eqref{eq:HyperbolicPDE_D_char_freq}, (iii) using Briggs' criterion~\cite{Briggs_1964_Electron} to classify the eigenvectors as left or right-going, and (iv) using this information to construct a one-way equation for the right-going waves. Since computing the eigendecomposition is computationally expensive, the \textit{approximate} formulation instead uses a \textit{recursive filter} to construct an approximate one-way equation, which converges to the exact one-way equation if its \textit{recursion parameters} are well-chosen. If the location (in the complex plane) and direction (left- or right-going) of all eigenvalues are known, then it is always possible to choose convergent recursion parameters. However, since the recursive filter is used to avoid computing the eigendecomposition of $M$, convergent recursion parameters must be chosen without complete knowledge of the eigenvalues of $M$. For example, Towne and Colonius addressed this issue for the Euler equations by using the eigenvalues of the Euler equations linearized about a uniform flow (which can be computed analytically) to inform recursion parameter selection for nonuniform flows. Throughout this paper, we refer to this as \textit{heuristic} recursion parameter selection.

OWNS-O can only be applied to homogeneous (unforced) equations, motivating the development of the OWNS projection (OWNS-P)~\cite{Towne_2022_OWNS-P} and OWNS recursive (OWNS-R)~\cite{Zhu_2021_OWNS-R} approaches, which can be applied to inhomogeneous (forced) equations. We note that OWNS-P and OWNS-R use recursive filters to obtain approximate one-way equations, which differ from each other but are both approximations to the same \textit{exact} one-way equation (which is also different from the \textit{exact} OWNS-O equation). We further note that note that Rudel et al.~\cite{Rudel_2022_Bremmer} have also used recursive filters to construct one-way equations for wave propagation in complex media, which can also be applied to inhomogeneous equations, while Sleeman et al.~\cite{Sleeman_2025_NOWNS_AIAAJ} have developed the nonlinear OWNS (NOWNS) approach, based on OWNS-P, which enables nonlinear disturbance evolution in boundary layers.

%The recursive filter requires carefully chosen \textit{recursion parameters} to ensure that left-going waves are filtered out while right-going waves are retained. If the location (in the complex plain) and direction (left- or right-going) of all eigenvalues are known, then it is always possible to choose convergent recursion parameters. However, the eigenvalues will generally be unknown since the recursive filter is used to avoid computing the eigendecomposition. Towne and Colonius applied their method to the Euler equations, where they used the eigenvalues of the Euler equations linearized about uniform flows (which can be computed analytically) to inform recursion parameter selection for nonuniform flows, demonstrating their method to be accurate relative to analytical and direct numerical simulation solutions.

%To avoid factoring the dispersion relation, Towne and Colonius first discretize in the transverse directions to obtain a semi-discrete ordinary differential equation (ODE) in the marching direction~\eqref{eq:HyperbolicPDE_D_char_freq}. Then they take the eigendecomposition of the discretized linear operator, $M$ in~\eqref{eq:HyperbolicPDE_D_char_freq}, and use Briggs' criterion~\cite{Briggs_1964_Electron} to classify its eigenvectors as left- or right-going, allowing for the construction of a one-way equation. Computing the eigendecomposition is computationally expensive, so they instead use a \textit{recursive filter}, based on work by Givoli and Neta~\cite{Givoli_2003_NRBC} and Hagstrom and Warburton~\cite{Hagstrom_2004_NRBC} for non-reflective boundary conditions (NRBCs) at ``outflow'' boundaries, to construct an approximate one-way equation, which requires carefully chosen \textit{recursion parameters} to ensure convergence, stability, and accuracy. If the location (in the complex plain) and direction (left- or right-going) of all eigenvalues are known, then it is always possible to choose convergent recursion parameters. However, the eigenvalues will generally be unknown since the recursive filter is used to avoid computing the eigendecomposition. Towne and Colonius applied their method to the Euler equations, where they used the eigenvalues of the Euler equations linearized about uniform flows (which can be computed analytically) to inform recursion parameter selection for nonuniform flows, demonstrating their method to be accurate relative to analytical and direct numerical simulation solutions.

%Unlike the Euler equations, the Navier-Stokes equations are not hyperbolic. Nevertheless, the One-Way Navier-Stokes (OWNS) framework generalizes the approach of Towne and Colonius to the linearized Navier-Stokes equations~\cite{Towne_2022_OWNS-P,Kamal_2020_HOWNS,Kamal_2021_OWNS_IO,Kamal_2022_HOWNS,Rigas_2017_OWNS_BL}, where it has been used for linear disturbance evolution in turbulent jets and boundary layers. When applied to the Navier-Stokes equations, the method is now referred to as the OWNS outflow (OWNS-O) approach since it is based on recursive filters for outflow boundaries. OWNS-O can only be applied to homogeneous (unforced) equations, which motivated the development of the OWNS projection (OWNS-P)~\cite{Towne_2022_OWNS-P} and OWNS recursive (OWNS-R)~\cite{Zhu_2021_OWNS-R} approaches for inhomogeneous (forced) equations. Finally, we note that Rudel et al.~\cite{Rudel_2022_Bremmer} have also used recursive filters to construct one-way equations for wave propagation in complex media, while Sleeman et al.~\cite{Sleeman_2025_NOWNS_AIAAJ} have developed the nonlinear OWNS (NOWNS) approach, based on OWNS-P, which enables nonlinear disturbance evolution in boundary layers. For simplicity, we will use the OWNS label even when we are analyzing hyperbolic equations.

Although the \textit{heuristic} recursion parameter selection routine developed by Towne and Colonius has proven effective for the Euler and Navier-Stokes equations, it requires problem-specific parameter tuning (e.g., subsonic and supersonic boundary layer flows require different parameters). Therefore, we propose a \textit{greedy} algorithm for automatic recursion parameter selection that can be applied to any system of first-order hyperbolic equations, as well as the Navier-Stokes equations. Unlike the \textit{heuristic} approach, we compute and label the eigenvalues of $M$. Then we randomly select one left- and one right-going eigenvalue as the initial parameter set and use a bound on the recursive filter error to iteratively append the worst-approximated left- and right-going eigenvalues until convergence. We demonstrate for linear and nonlinear disturbance evolution in subsonic and supersonic boundary-layer flows (modeled using the Navier-Stokes equations) that the greedy approach yields faster filter convergence than the heuristic approach, while also leading to a reduced computational cost. We further show for nonlinear disturbance evolution in a supersonic boundary layer flow that while the heuristic selection contaminates the solution with numerical error, this problem is alleviated using greedy selection.

We additionally analyze the convergence properties for the OWNS-P and OWNS-R methods, showing that OWNS-P has superior convergence properties and that there exist recursion parameter sets for which OWNS-P is fully converged while OWNS-R is not. Consistent with our analysis, we find in our numerical experiments that there exist recursion parameter sets for which OWNS-P achieves a stable and accurate march while OWNS-R does not. In particular, we find that although our heuristic parameter selection routine has been demonstrated to yield stable and accurate OWNS-P marches for a wide variety of boundary-layer flows~\cite{Towne_2015_OWNS-O,Towne_2022_OWNS-P,Kamal_2020_HOWNS,Rigas_2017_OWNS_BL}, it does not generally yield stable OWNS-R marches for these flows. In contrast, greedy selection yields stable and accurate OWNS-R marches.

Section~\ref{sec:OWNS} reviews the OWNS-P and OWNS-R methods and analyzes their stability and convergence properties, while Section~\ref{sec:greedy} presents the greedy algorithm for recursion parameter selection. We present the Navier-Stokes equations in~\ref{sec:NSE}, followed by convergence results at a single station in  Section~\ref{sec:greedySingle}, and spatial marching results in  Section~\ref{sec:greedyMarching}.


\section{OWNS equations}\label{sec:OWNS}

For an arbitrary system of linear first-order hyperbolic equations, Section~\ref{sec:global} develops the global equations we wish to approximate using one-way equations. Section~\ref{sec:Briggs} presents Briggs' criterion~\cite{Briggs_1964_Electron}, which we use to identify right- and left-going modes, while Section~\ref{sec:exact} uses this criterion to develop the exact one-way projection equations. Constructing the projection operator introduces numerical error while also entailing a high computational cost, so OWNS-P~\cite{Towne_2022_OWNS-P} and OWNS-R~\cite{Zhu_2021_OWNS-R} approximate its action using a rapidly-convergent \textit{recursive filter}. We conclude by reviewing OWNS-P and OWNS-R in Sections~\ref{subsec:OWNSP} and~\ref{subsec:OWNSR}, respectively, referring to Towne et al.~\cite{Towne_2022_OWNS-P} and Zhu and Towne~\cite{Zhu_2021_OWNS-R} for details on implementation. Throughout this paper, $\|\bm{b}\|$ for $\bm{b}\in\mathbb{R}^N$ denotes the Euclidean vector norm and $\|A\|$ for $A\in\mathbb{R}^{N\times N}$ denotes its induced matrix norm.

\subsection{Global equations}\label{sec:global}
Consider a system of first-order hyperbolic equations
\begin{subequations}
    \begin{align}
    \frac{\partial \bm{q}'}{\partial t}+A\frac{\partial \bm{q}'}{\partial x}+\sum_{j=2}^{d}B_j\frac{\partial \bm{q}'}{\partial y_j}+C \bm{q}'=\bm{f}\quad&\mathrm{on}\quad\Omega\times I\label{eq:HyperbolicPDE}\\
    \bm{q}'=\bm{q}_0'\quad&\mathrm{on}\quad\Omega\times\{t=0\}\\
    \bm{q}'=\bm{q}_{\mathrm{BC}}'\quad&\mathrm{on}\quad\partial\Omega\times I,
    \end{align}
    \label{eq:HyperbolicSystem}
\end{subequations}
in $d$ spatial-dimensions over the domain $\Omega\subset\mathbb{R}^d$ and time interval $I\equiv (0,T]$ for $T>0$, where $A,B_{j},C\in\mathbb{R}^{N\times N}$ for $j=2,\dots,d$, and $\bm{q}':\Omega\times I\to\mathbb{R}^N$ is the solution for initial condition $\bm{q}_0':\Omega\to\mathbb{R}^N$, boundary condition $\bm{q}_{\mathrm{BC}}':\partial\Omega\times I\to\mathbb{R}^N$, and forcing $\bm{f}:\Omega\times I\to\mathbb{R}^N$. If~\eqref{eq:HyperbolicSystem} is hyperbolic, then for all $\gamma_1,\dots,\gamma_d\in\mathbb{R}$ the matrix $\gamma_1A+\gamma_2B_2+\cdots+\gamma_dB_d$ has only real eigenvalues and is diagonalizable. Taking $\gamma_2=\cdots=\gamma_d=0$, we see that $A$ is diagonalizable with real eigenvalues $\tilde{A}=TAT^{-1}$ and eigenvectors $T^{-1}$, which use to define the characteristic variables $\bm{\phi}=T\bm{q}'$. Applying finite differences with appropriate boundary conditions in $(y_2,\dots,y_d)$ to~\eqref{eq:HyperbolicSystem} and transforming to characteristic variables yields
\begin{subequations}
\begin{align}
    \frac{\partial \bm{\phi}'}{\partial t}+\tilde{A}\frac{\partial \bm{\phi}'}{\partial x}+\sum_{j=2}^{d}\tilde{B}_j\mathcal{D}_j\bm{\phi}'+\tilde{C} \bm{\phi}'=\bm{f}_{\phi}\quad&\mathrm{on}\quad[x_L,x_R]\times I\\
    \bm{q}'=\bm{q}_0'\quad&\mathrm{on}\quad[x_L,x_R]\times\{t=0\}\\
    \bm{q}'=\bm{q}_L\quad&\mathrm{on}\quad \{x=x_L\}\times I\\
    \bm{q}'=\bm{q}_R\quad&\mathrm{on}\quad \{x=x_R\}\times I
\end{align}
\label{eq:HyperbolicPDE_D_char}
\end{subequations}
for the difference operator $\mathcal{D}_j$ where $L$ and $R$ denote values associated with the left and right boundaries, respectively, $\bm{f}_{\phi}=T\bm{f}$, $\tilde{B}_j=TB_jT^{-1}$ for $j=2,\dots,d$ and $\tilde{C}=TCT^{-1}+TA \partial T^{-1} /\partial x$.
We assume that $\tilde{A}$ is non-singular (the singular case is treated by Towne et al.\cite{Towne_2022_OWNS-P}) so that $\tilde{A}=\diag(A_{++},A_{--})$, where $A_{++}\in\mathbb{R}^{N_+\times N_+}$ and $A_{--}\in\mathbb{R}^{N_-\times N_-}$ are positive and negative diagonal matrices, respectively. We take a Laplace transform in time to obtain
\begin{equation}
\frac{d \hat{\bm{\phi}}}{d x} = M(s)\hat{\bm{\phi}} + \hat{\bm{g}},
\quad x \in [x_L, x_R],
\quad \hat{\bm{\phi}}(x_L) = \hat{\bm{\phi}}_{L}, 
\quad \hat{\bm{\phi}}(x_R) = \hat{\bm{\phi}}_{R}.
    \label{eq:HyperbolicPDE_D_char_freq}
\end{equation}
% \begin{subequations}
% \begin{align}
%     \frac{d \hat{\bm{\phi}}}{d x}=M(s)\hat{\bm{\phi}}+\hat{\bm{g}}\quad&\mathrm{on}\quad[x_L,x_R]\\
%     \hat{\bm{\phi}}=\hat{\bm{\phi}}_{L}\quad&\mathrm{on}\quad\{x=x_L\}\\
%     \hat{\bm{\phi}}=\hat{\bm{\phi}}_{R}\quad&\mathrm{on}\quad\{x=x_R\},
% \end{align}
% \end{subequations}
where $\hat{\bm{\phi}}$ and $\hat{\bm{g}}$ represent $\bm{\phi}$ and $\bm{f}_{\phi}$, respectively, in the frequency domain, while $M(s) = -\tilde{A}^{-1} \big(s I + \sum_{j=2}^{d} \mathcal{D}_j \tilde{B}_j + \tilde{C} \big)$. We assume that $M(s)$ is diagonalizable with eigenpairs $\{i\alpha_k(s),\bm{v}_k(s)\}_{k=1}^N$ since this is true for the problems we consider, while also simplifying the exposition (the defective case is handled by Towne and Colonius~\cite{Towne_2015_OWNS-O}).


\subsection{Briggs' criterion}\label{sec:Briggs}

Briggs' criterion (Definition~\ref{def:Briggs}) classifies eigenpairs as either left- or right-going, while Proposition~\ref{prop:Briggs} shows that $M(s)$ has precisely $N_+$ and $N_-$ right- and left-going eigenvalues, respectively.

\begin{definition}[Briggs' criterion~\cite{Briggs_1964_Electron}]
    Consider a wave (eigenvector) with complex wave-number (eigenvalue) $i\alpha$ for some $s=i\omega+\eta$ with $\omega,\eta\in\mathbb{R}$. This wave is right-going if $\lim_{\eta\to\infty}\mathcal{I}[\alpha(s)]\to\infty$, while it is left-going if $\lim_{\eta\to\infty}\mathcal{I}[\alpha(s)]\to-\infty$.
    \label{def:Briggs}
\end{definition}


\begin{proposition}\label{prop:Briggs}
For $\mathcal{R}(s)>0$, the matrix $M(s)$ has precisely $N_{+}$ and $N_{-}$ right- and left-going eigenvalues, respectively, according on Briggs' criterion~\cite{Towne_2015_OWNS-O}.
\end{proposition}
\begin{proof}
    The eigenvalues are the solutions to the characteristic equation
    \[
    \det(\tilde{A}^{-1})^{-1}\det(M-i\alpha I)
    =\det(-sI-\sum_{j=2}^{d}\mathcal{D}_j \tilde{B}_j-i\alpha \tilde{A}-\tilde{C}).
    \]
    The eigenvalues are continuous functions of $s$ so taking the limit $\eta\to\infty$ yields $N_{+}$ eigenvalues with $\mathcal{I}[\alpha(s)]\to\infty$ and $N_{-}$ eigenvalues with $\mathcal{I}[\alpha(s)]\to-\infty$.
\end{proof}

We partition $M$ into left- and right-going blocks as
\begin{equation}
    M
    =
    \begin{bmatrix}
        V_+ & V_-
    \end{bmatrix}
    \begin{bmatrix}
        D_{++} & 0\\
        0 & D_{--}
    \end{bmatrix}
    \begin{bmatrix}
        V_+ & V_-
    \end{bmatrix}^{-1},
\end{equation}
where the columns of $V_+\in\mathbb{C}^{N\times N_+}$ and $V_-\in\mathbb{C}^{N\times N_-}$ the eigenvectors associated with the right- and left-going eigenvalues of $M$, and we have dropped the argument $s$ for brevity. Next we introduce the coefficients $\hat{\bm{\psi}}_+\in\mathbb{C}^{N_+}$ and $\hat{\bm{\psi}}_-\in\mathbb{C}^{N_-}$ such that $\hat{\bm{\phi}} = V_+\hat{\bm{\psi}}_+ + V_-\hat{\bm{\psi}}_-$, and we note that for any eigenvector $\bm{v}_k$, we have $\bm{v}_k=V\hat{\bm{\psi}}^{(k)}$ with $\hat{\psi}_l^{(k)}=\delta_{lk}$ for $k,l=1,\dots,N$ , where $\delta_{lk}$ is the Kronecker delta. For notational convenience, we define the sets $i^{(+)}=\{1,\dots,N_+\}$ and $i^{(-)}=\{N_++1,\dots,N\}$ to denote indices associated with right- and left-going modes, respectively. The eigenvalues need not be unique, so we introduce $\tilde{N}_+\leq N_+$ and $\tilde{N}_-\leq N_-$ to denote the number of unique right- and left-going eigenvalues, respectively. We additionally introduce the subsets
$\tilde{i}^{(+)}=\{ \tilde{i}^{(+)}(1),\dots,\tilde{i}^{(+)}(\tilde{N}_+)\}\subset i^{(+)}$ and
$\tilde{i}^{(-)}=\{ \tilde{i}^{(-)}(1),\dots,\tilde{i}^{(-)}(\tilde{N}_-)\}\subset i^{(-)}$ which denote the indices of the unique right- and left-going eigenvalues.

\subsection{Exact projection equations}\label{sec:exact}

Definitions~\ref{def:proj_mat} and~\ref{def:proj_eq} introduce the exact projection matrix and its associated one-way equations, while Proposition~\ref{prop:projection-consistent-2} provides conditions under which the global equation~\eqref{eq:HyperbolicPDE_D_char_freq} can be solved using two one-way equations and Proposition~\ref{prop:projection-consistent} shows that the sum of the two one-way solutions are always solution to the global equation (OWNS-O does not have this property~\cite{Towne_2015_OWNS-O}).

\begin{definition}\label{def:proj_mat}
We define the one-way projection matrix
\begin{equation}
P = V E V^{-1},\quad E=\begin{bmatrix}
    I_{++} & 0\\
    0 & 0
\end{bmatrix}
%\label{eq:projectionOp}
\end{equation}
which partitions the solution into right- and left-going components as $\hat{\bm{\phi}}'=P\hat{\bm{\phi}}=V_+\psi_+$ and $\hat{\bm{\phi}}''=[I-P]\hat{\bm{\phi}}=V_-\hat{\bm{\psi}}$, respectively, where $I_{++}\in\mathbb{R}^{N_+\times N_+}$ is the identity.
\end{definition}
\begin{proposition}
    $P$ is a projection matrix~\cite{Towne_2022_OWNS-P}.
\end{proposition}
\begin{proof}
    $E^2=E$ so that $P^2=VEV^{-1}VEV^{-1}=VEV^{-1}=P$.
\end{proof}
\begin{proposition}
    $P$ commutes with $M$~\cite{Towne_2022_OWNS-P}.
    \label{prop:commutativity}
\end{proposition}
\begin{proof}
    $ED=DE$ so that $PM=VEDV^{-1}=VDEV^{-1}=MP$.
\end{proof}
% \begin{definition}[exact one-way projection equations~\cite{Towne_2022_OWNS-P}]\label{def:proj_eq}
% The projection matrix, $P$, can be used to obtain the one-way equations for the right-
%     \begin{subequations}
%     \begin{align}
%         \frac{\partial \hat{\bm{\phi}}'}{\partial x}
%         =P[M\hat{\bm{\phi}}'+\hat{\bm{g}}],\quad&\mathrm{on}\quad[x_L,x_R]\\
%         \hat{\bm{\phi}}'=\hat{\bm{\phi}}_L,\quad&\mathrm{on}\quad\{x=x_L\}
%     \end{align}
%     \label{eq:lowns1}
%     \end{subequations}
%     and left-going modes
%     \begin{subequations}
%     \begin{align}
%         \frac{\partial \hat{\bm{\phi}''}}{\partial x}
%         =[I-P][M\hat{\bm{\phi}}''+\hat{\bm{g}}],\quad&\mathrm{on}\quad[x_L,x_R]\\
%         \hat{\bm{\phi}}''=\hat{\bm{\phi}}_R,\quad&\mathrm{on}\quad\{x=x_R\},
%     \end{align}
%     \label{eq:lowns2}
%     \end{subequations}
%     respectively.
% \end{definition}
\begin{definition}[Exact one-way projection equations~\cite{Towne_2022_OWNS-P}]\label{def:proj_eq}
The projection matrix $P$ yields the one-way equations for right- and left-going modes:
\begin{subequations}
\begin{align}
\frac{\partial \hat{\bm{\phi}}'}{\partial x}
= P[M\hat{\bm{\phi}}' + \hat{\bm{g}}],
\quad x \in [x_L,x_R],
\quad \hat{\bm{\phi}}'(x_L) = \hat{\bm{\phi}}_L', \label{eq:lowns1} \\
\frac{\partial \hat{\bm{\phi}}''}{\partial x}
= (I-P)[M\hat{\bm{\phi}}'' + \hat{\bm{g}}],
\quad x \in [x_L,x_R],
\quad \hat{\bm{\phi}}''(x_R) = \hat{\bm{\phi}}_R''. \label{eq:lowns2}
\end{align}
\label{eq:lowns}
\end{subequations}
\end{definition}


\begin{proposition} The exact one-way projection equations are well-posed as spatial initial value problems according to the criterion of Kreiss~\cite{Kreiss_1970_IBVP}.
\end{proposition}
\begin{proof}
    See the discussion by Towne and Colonius~\cite{Towne_2015_OWNS-O} and Towne et al.~\cite{Towne_2022_OWNS-P}, the criterion developed by Kreiss~\cite{Kreiss_1970_IBVP}, and the review of this criterion by Higdon~\cite{Higdon_1986_Kreiss}.
\end{proof}

\begin{proposition}\label{prop:projection-consistent-2}
    If $\hat{\bm{\phi}}$ satisfies~\eqref{eq:HyperbolicPDE_D_char_freq} with $\hat{\bm{\phi}}(x_L)=\hat{\bm{\phi}}_L$ and $\hat{\bm{\phi}}(x_R)=\hat{\bm{\phi}}_R$,    
    then $P\hat{\bm{\phi}}$ and $[I-P]\hat{\bm{\phi}}$ satisfy~\eqref{eq:lowns1} and~\eqref{eq:lowns2} with $\hat{\bm{\phi}}'(x_L)=P\hat{\bm{\phi}}_L$ and $\hat{\bm{\phi}}''(x_R)=[I-P]\hat{\bm{\phi}}_R$ if and only if $[I-P]\partial \bm{\phi}' / \partial x=0$ and $P \partial \bm{\phi}'' / \partial x=0$.
\end{proposition}
\begin{proof}
    Pre-multiply~\eqref{eq:HyperbolicPDE_D_char_freq} by $P$ and $[I-P]$ to obtain
    \begin{align*}
    P \frac{\partial (P\hat{\bm{\phi}})}{\partial x} + P \frac{\partial ([I-P]\hat{\bm{\phi}})}{\partial x} 
    &= P [M \hat{\bm{\phi}}' + \hat{\bm{g}}],\\
    [I-P] \frac{\partial (P\hat{\bm{\phi}})}{\partial x} + [I-P] \frac{\partial ([I-P]\hat{\bm{\phi}})}{\partial x} 
    &= [I-P] [M \hat{\bm{\phi}}'' + \hat{\bm{g}}].
    \end{align*}
    If $[I-P]\partial \bm{\phi}' / \partial x=0$ and $P \partial \bm{\phi}'' / \partial x=0$, then we obtain~\eqref{eq:lowns1} and~\eqref{eq:lowns2}, respectively, with stated boundary conditions. Otherwise, we do not recover~\eqref{eq:lowns1} and~\eqref{eq:lowns2}.
\end{proof}

\begin{proposition}\label{prop:projection-consistent}
If $\hat{\bm{\phi}}'$ and $\hat{\bm{\phi}}''$ satisfy~\eqref{eq:lowns1} and~\eqref{eq:lowns2} with 
$\hat{\bm{\phi}}'(x_L)=\hat{\bm{\phi}}_L'$ and $\hat{\bm{\phi}}''(x_R)=\hat{\bm{\phi}}_R''$, 
then $\hat{\bm{\phi}}'+\hat{\bm{\phi}}''$ satisfies~\eqref{eq:HyperbolicPDE_D_char_freq} with 
$\hat{\bm{\phi}}(x_L) = \hat{\bm{\phi}}_L'+ \hat{\bm{\phi}}''_L$ 
and $\hat{\bm{\phi}}(x_R) = \hat{\bm{\phi}}'_R + \hat{\bm{\phi}}''_R$.
\end{proposition}
\begin{proof}
Summing~\eqref{eq:lowns1} and~\eqref{eq:lowns2} gives $\partial(\hat{\bm{\phi}}'+\hat{\bm{\phi}}'')/\partial x=M[\hat{\bm{\phi}}'+\hat{\bm{\phi}}'']+\hat{\bm{g}}$, showing that $\hat{\bm{\phi}}'+\hat{\bm{\phi}}''$ satisfies~\eqref{eq:HyperbolicPDE_D_char_freq} 
with the stated boundary conditions.
\end{proof}

Proposition~\ref{prop:projection-consistent} shows that, unlike OWNS-O~\cite{Towne_2015_OWNS-O}, solutions to the one-way projection equations~\eqref{eq:lowns} are also solutions to the global equations~\eqref{eq:HyperbolicPDE_D_char_freq}. As discussed by Towne et al.~\cite{Towne_2022_OWNS-P}, disturbance evolution in spatially-developing fluid flows (e.g., boundary layers and jets) can be modeled using one-way equations, even though the conditions of Proposition~\eqref{prop:projection-consistent-2} do not hold (all of $\partial P/\partial x$, $\bm{\phi}'$, and $\bm{\phi}''$ are non-zero), because the flow evolves slowly in $x$ and the disturbances propagate primarily in one direction so that both $\partial P/\partial x$ and $\hat{\bm{\phi}}''$ are small.


\subsection{Approximate projection using OWNS-P}~\label{subsec:OWNSP}
Definition~\ref{def:filter-p} introduces the OWNS-P filter, while Proposition~\ref{prop:proj} recasts it in a matrix form, which we show to be a projection in Proposition~\ref{prop:approxProjectProjectMat}. Proposition~\ref{prop:approxProjectConvergence} provides a criterion for filter convergence, while Proposition~\ref{prop:minimalOWNS-P} provides a necessary and sufficient condition for convergent recursion parameters to exist. We bound the error introduced by this approximation in Proposition~\ref{prop:errOWNSP}, while Proposition~\ref{prop:commMatOWNS-P} shows that unlike the exact projection matrix, the approximate projection matrix does not commute with $M$, unless the approximation is fully converged.


\begin{definition}[OWNS-P filter]\label{def:filter-p}
    Given the solution $\hat{\bm{\phi}}$, OWNS-P returns the filtered solution $\hat{\bm{\phi}}^0$ by solving
    \begin{subequations}
    \begin{align}
        \hat{\bm{\phi}}_+^{-N_\beta}&=0 \label{eq:filterEndPlus}\\
        (M-i\beta_-^j I)\hat{\bm{\phi}}^{-j}-(M-i\beta_+^j I)\hat{\bm{\phi}}^{-j-1}&=0,\quad j=1,\dots,N_\beta-1,\label{eq:filterTop}\\
        (M-i\beta_-^0 I)\hat{\bm{\phi}}^{0}-(M-i\beta_+^0 I)\hat{\bm{\phi}}^{-1}&=(M-i\beta_-^0I)\hat{\bm{\phi}},\label{eq:filterMiddle}\\
        (M-i\beta_+^j I)\hat{\bm{\phi}}^{j}-(M-i\beta_-^j I)\hat{\bm{\phi}}^{j+1}&=0,\quad j=0,\dots,N_\beta-1,\label{eq:filterBottom}\\
        \hat{\bm{\phi}}_-^{N_\beta}&=0 \label{eq:filterEndMinus},
    \end{align}
    \label{eq:filter-p}
    \end{subequations}
    using the recursion parameters $\{\beta_+^j,\beta_-^j\}_{j=0}^{N_\beta-1}$ and auxiliary variables $\{\hat{\bm{\phi}}^j\}_{j=0}^{N_\beta}$~\cite{Towne_2022_OWNS-P}.
\end{definition}
\begin{proposition}\label{prop:proj} The recursive filter~\eqref{eq:filter-p} can be recast in matrix form
\begin{subequations}
    \begin{equation}
    P_{N_\beta}
    =
    V R_{N_\beta}E R_{N_\beta}^{-1}V^{-1},
    \label{eq:approxProject}
    \end{equation}
    for
    \begin{equation}
    R_{N_\beta}
    =
    \begin{bmatrix}
        I_{++} & F_{++}V_{++}^{-1}V_{+-}F_{--}^{-1}\\
        F_{--}^{-1}V_{--}^{-1}V_{-+}F_{++} & I_{--}
    \end{bmatrix}^{-1},
    \label{eq:approxProjectRMat}
    \end{equation}
    where $F$ is the diagonal matrix
    \begin{equation}
        F
        =\begin{bmatrix}
            F_{++} & 0\\
            0 & F_{--}
        \end{bmatrix},\quad
        F_k=\prod_{j=0}^{N_\beta-1} \frac{\alpha_k - \beta_+^j}{\alpha_k- \beta_-^{j}},\quad k=1,\dots,N.
        \label{def:matF}
    \end{equation}
    \end{subequations}
\end{proposition}
\begin{proof}
    See Towne et al.~\cite{Towne_2022_OWNS-P}.
\end{proof}

\begin{proposition}\label{prop:approxProjectProjectMat}
$P_{N_\beta}$ is a projection matrix~\cite{Towne_2016_Thesis}.
\end{proposition}
\begin{proof}
$P_{N_\beta}^2
=VR_{N_\beta}E R_{N_\beta}^{-1}V^{-1}VR_{N_\beta}ER_{N_\beta}^{-1}V^{-1}
=P_{N_\beta}$.
\end{proof}

\begin{proposition}\label{prop:approxProjectConvergence} $P_{N_\beta}\to P$ if and only if
    \begin{equation}
    \lim_{N_\beta\to\infty}
    \prod_{j=0}^{N_\beta-1}\frac{|\alpha_m-\beta_{+}^{j}|}
    {|\alpha_m-\beta_-^j|}
    \frac{|\alpha_n-\beta_{-}^{j}|}
    {|\alpha_n-\beta_+^j|}=0,\quad\forall(m,n)\in i^{(+)}\times i^{(-)}.
    \label{eq:fConv}
    \end{equation}
\end{proposition}
\begin{proof}
    See Towne et al.~\cite{Towne_2022_OWNS-P}.
\end{proof}

\begin{proposition}\label{prop:minimalOWNS-P}
\begin{subequations}
    Recursion parameters such that $P_{N_\beta}\to P$ exist if and only if $\alpha_m\neq\alpha_n$ for all $(m,n)\in i^{(+)}\times i^{(-)}$. If $\tilde{N}_+\leq \tilde{N}_-$, then take $N_\beta=\tilde{N}_+$ with
    \begin{equation}
        \beta_+^{j-1}=\alpha_{\tilde{i}^{(+)}(j)},\quad
        \beta_-^{j-1}\neq\alpha_{\tilde{i}^{(+)}(j)},\quad
        j=1,\dots,\tilde{N}_+,
        \label{eq:owns-p-params_plus}
    \end{equation}
    while if $\tilde{N}_-<\tilde{N}_+$, then take $N_\beta=\tilde{N}_-$ with
    \begin{equation}
        \beta_-^{j-1}=\alpha_{\tilde{i}^{(-)}(j)},\quad
        \beta_+^{j-1}\neq\alpha_{\tilde{i}^{(-)}(j)},\quad
        j=1,\dots,\tilde{N}_-.
        \label{eq:owns-p-params_minus}
    \end{equation}
\end{subequations}
\end{proposition}
\begin{proof}
    If there exists $(\hat{m},\hat{n})\in i^{(+)}\times i^{(-)}$ such that $\alpha_{\hat{m}}=\alpha_{\hat{n}}$, then
    \[
    R_{\hat{n}\hat{m}}
    =
    \prod_{j=0}^{N_\beta-1}\frac{\alpha_{\hat{m}}-\beta_{+}^{j}}{\alpha_{\hat{m}}-\beta_-^j}\frac{\alpha_{\hat{n}}-\beta_{-}^{j}}{\alpha_{\hat{n}}-\beta_+^j}
    (V_{--}^{-1}V_{-+})_{\hat{n}\hat{m}}
    =
    (V_{--}^{-1}V_{-+})_{\hat{n}\hat{m}}
    \neq 0
    \]
    so that $R_{N_\beta}^{-1}\neq I$ for any choice of recursion parameters.

    Next assume that $\alpha_m\neq\alpha_n$ for all $(m,n)\in i^{(+)}\times i^{(-)}$, and note that
    \[
    \max_{(m,n)\in i^{(+)}\times i^{(-)}}\prod_{j=0}^{N_\beta-1}\frac{|\alpha_m-\beta_{+}^{j}|}
    {|\alpha_m-\beta_-^j|}
    \frac{|\alpha_n-\beta_{-}^{j}|}
    {|\alpha_n-\beta_+^j|}=\|F_{++}\|\|F_{--}^{-1}\|,
    \]
    so that by Proposition~\ref{prop:approxProjectConvergence}, $P_{N_\beta}\to P$ when $\|F_{++}\|\|F_{--}^{-1}\|=0$. If $\tilde{N}_+\leq \tilde{N}_-$, we use~\eqref{eq:owns-p-params_plus} to obtain $\|F_{++}\|=0$ since $\alpha_m\in\{\beta_j\}_{j=0}^{N_\beta-1}$ and $\alpha_m\notin\{\beta_j\}_{j=0}^{N_\beta-1}$ for all $m \in i^{(+)}$. If $\tilde{N}_- < \tilde{N}_+$, we use~\eqref{eq:owns-p-params_minus} to obtain $\|F_{--}^{-1}\|=0$ since $\alpha_n\in\{\beta_-^j\}_{j=0}^{N_\beta-1}$ and $\alpha_n\notin\{\beta_+^j\}_{j=0}^{N_\beta-1}$ for all $n\in i^{(-)}$.
\end{proof}

Proposition~\ref{prop:errBoundTriangle} bounds $\|E-R_{N_\beta}^{-1}ER_{N_\beta}\|$, while Proposition~\ref{prop:errOWNSP} uses this result to bound $\|P-P_{N_\beta}\|$ for small $\|F_{++}\|\|F_{--}^{-1}\|$.

\begin{proposition}\label{prop:errBoundTriangle}
    The norm of $E-R_{N_\beta}ER_{N_\beta}^{-1}$ can be bounded
    \begin{align}
    \begin{split}
        \|E-R_{N_\beta}ER_{N_\beta}^{-1}\|&\leq \|(I_{++}-F_{++}V_{++}^{-1}V_{+-}F_{--}^{-2}V_{--}^{-1}V_{-+}F_{++})^{-1}\|\\
        &\big(
        2\|F_{++}\|^2\|F_{--}^{-1}\|^2\|V_{++}^{-1}V_{+-}\|\|V_{--}^{-1}V_{-+}\|\\
        &+\|F_{++}\|\|F_{--}^{-1}\|\|V_{++}^{-1}V_{+-}\|+\|F_{++}\|\|F_{--}^{-1}\|\|V_{--}^{-1}V_{-+}\|\|
        \big).
    \end{split}
    \label{eq:errBoundTriangleFinal}
    \end{align}
\end{proposition}
\begin{proof}
    See Appendix~\eqref{app:Proofs}
\end{proof}

\begin{proposition}\label{prop:errOWNSP}
    If $\|F_{++}\|\|F_{--}^{-1}\|<\epsilon$ where
    \begin{equation}
        \epsilon \equiv \min\{\hat{\epsilon},\|V_{++}^{-1}V_{+-}\|^{-1/2}\|V_{--}^{-1}V_{-+}\|^{-1/2}\}
    \label{eq:errBoundEpsilon}
    \end{equation}
    for small $\hat{\epsilon}>0$, then the OWNS-P error is bounded as
    \begin{equation}
        \|P_{N_\beta}-P\|\leq \|V\|\|F_{++}\|\|F_{--}^{-1}\|\big(\|V_{++}^{-1}V_{+-}\|+\|V_{--}^{-1}V_{-+}\|\big)\|V^{-1}\|+\mathcal{O}(\epsilon^2).
        \label{eq:errOWNSP}
    \end{equation}
\end{proposition}
\begin{proof}
    By the triangle inequality
    \[
    \|\sum_{k=0}^\infty(F_{++}V_{++}^{-1}V_{+-}F_{--}^{-2}V_{--}^{-1}V_{-+}F_{++})^k\|\leq \sum_{k=0}^\infty a^k
    \]
    for $a=\|F_{++}\|^2\|F_{--}^{-1}\|^2\|V_{++}^{-1}V_{+-}\|\|V_{--}^{-1}V_{-+}\|$, so that the Neumann series converges if $a<1$, or $\|F_{++}\|\|F_{--}^{-1}\|<(\|V_{++}^{-1}V_{+-}\|\|V_{--}^{-1}V_{-+}\|)^{-1/2}$. For $\|F_{++}\|\|F_{--}^{-1}\|<\epsilon$, with $\epsilon$ as in~\eqref{eq:errBoundEpsilon}, we obtain
    \[
        \|(I_{++}-F_{++}V_{++}^{-1}V_{+-}F_{--}^{-2}V_{--}^{-1}V_{-+}F_{++})^{-1}\|\leq1+\mathcal{O}(\epsilon^2).
    \]
    so that using~\eqref{eq:errBoundTriangleFinal} from Proposition~\eqref{prop:errBoundTriangle} yields
    \begin{equation}
        \|E-R_{N_\beta}ER_{N_\beta}^{-1}\|\leq
        \|F_{++}\|\|F_{--}^{-1}\|\big(\|V_{++}^{-1}V_{+-}\|+\|V_{--}^{-1}V_{-+}\|\big)+\mathcal{O}(\epsilon^2).
        \label{eq:errBoundNeumann}
    \end{equation}
    Finally, the induced Euclidean norm of a matrix is sub-multiplicative so that
    \[
    \|P-P_{N_\beta}\|\leq \|V\| \|E-R_{N_\beta}ER_{N_\beta}^{-1}\| \|V^{-1}\|,
    \]
    which yields~\eqref{eq:errOWNSP} when combined with~\eqref{eq:errBoundNeumann}.
\end{proof}

\begin{proposition}~\label{prop:greedy-p}
    If $\alpha_m\in\{\beta_+^j\}_{j=0}^{N_\beta-1}$ for any $m\in i^{(+)}$, then $P_{N_\beta}\bm{v}_m=\bm{v}_m$; if $\alpha_n\in\{\beta_-^j\}_{j=0}^{N_\beta-1}$ for any $n\in i^{(-)}$, then $P_{N_\beta}\bm{v}_n=0$.
\end{proposition}

\begin{proof}
    Since $(M-i\beta I)\hat{\bm{\phi}}=V(D-i\beta I)\bm{\psi}$ where $V$ has full rank and $(D-i\beta I)$, we diagonalize~\eqref{eq:filterTop} through~\eqref{eq:filterBottom} as
    \begin{subequations}
    \begin{align}
        (\alpha_k-\beta_-^j)\hat{\psi}_{k}^{-j}-(\alpha_k-\beta_+^j)\hat{\psi}_{k}^{-j-1}&=0,\quad j=1,\dots,N_\beta-1,\label{eq:OWNS-P-diag-minus}\\
        (\alpha_k-\beta_-^0)\hat{\psi}_{k}^{0}-(\alpha_k-\beta_+^0)\hat{\psi}_{k}^{-1}&=(\alpha_k-\beta_-^0)\hat{\psi}_k,\label{eq:OWNS-P-diag-zero}\\
        (\alpha_k-\beta_+^j)\hat{\psi}_{k}^{j}-(\alpha_k-\beta_-^j)\hat{\psi}_{k}^{j+1}&=0,\quad j=0,\dots,N_\beta-1\label{eq:OWNS-P-diag-plus},
    \end{align}
    \end{subequations}
    for $k=1,\dots,N$. Using~\eqref{eq:OWNS-P-diag-minus} and~\eqref{eq:OWNS-P-diag-zero}, we see that
    \[
    \hat{\psi}_m^{0}-\hat{\psi}_m=\prod_{j=0}^{N_\beta-1}\frac{\alpha_m - \beta_+^j}{\alpha_m- \beta_-^{j}}\hat{\psi}_m^{-N_\beta}=0,
    \]
    since $\alpha_m\in\{\beta_+^j\}_{j=0}^{N_\beta-1}$. For $\bm{v}_m$, recall that $\hat{\psi}_m^{(l)}=\delta_{lm}$, so that $\hat{\psi}_m^{(m)0}=\hat{\psi}_m^{(m)}$ implies $P_{N_\beta}\bm{v}_m=\bm{v}_m$. Using~\eqref{eq:OWNS-P-diag-zero} and~\eqref{eq:OWNS-P-diag-plus}, we see that
    \[
    \hat{\psi}_n^0=\prod_{j=0}^{N_\beta-1}\frac{\alpha_n - \beta_-^j}{\alpha_n- \beta_+^{j}}\hat{\psi}_n^{ N_\beta}=0
    \]
    since $\alpha_n\in\{\beta_-^j\}_{j=0}^{N_\beta-1}$, so that for $\bm{v}_n$, $\hat{\psi}_n^{(n)0}=0$ implies $P_{N_\beta}\bm{v}_n=0$ since $\hat{\psi}_n^{(l)}=\delta_{ln}$.
\end{proof}

\begin{remark}
Proposition~\ref{prop:errOWNSP} shows that the error $\|P_{N_\beta}-P\|$ scales with $\|F_{++}\|\|F_{--}\|$ for small $\|F_{++}\|\|F_{--}\|$, while Proposition~\ref{prop:greedy-p} shows it is always possible to achieve zero error for any eigenvector by using its eigenvalue as a recursion parameter.
\end{remark}

\begin{proposition}\label{prop:commMatOWNS-P}
    $P_{N_\beta}$ commutes with $M$ if and only if $R_{N_\beta}^{-1}=I$.
\end{proposition}
\begin{proof}
    If $R_{N_\beta}^{-1}=I$, then $P_{N_\beta}M=PM=MP=MP_{N_\beta}$. If $R_{N_\beta}^{-1}\neq I$, then $R_{N_\beta}^{-1}D\neq D R_{N_\beta}$, and $P_{N_\beta}$ does not commute with $M$.
\end{proof}

\begin{remark}
Proposition~\ref{prop:approxProjectProjectMat} establishes that $P_{N_\beta}$ is always a projection matrix, while Proposition~\ref{prop:commMatOWNS-P}, in conjunction with Proposition~\ref{prop:approxProjectConvergence}, shows that although $P_{N_\beta}$ approximates $P$, it only commutes with $M$ if the approximation is fully converged.
\end{remark}

% \begin{proposition}\label{prop:errCommP}
%     The error in treating $P_{N_\beta}$ and $M$ as if they commute is bounded by
%     \begin{align}
%     \begin{split}
%         \|P_{N_\beta}M-MP_{N_\beta}\|
%         &\leq
%         2\|V\|\|F_{++}\|\|F_{--}^{-1}\|\big(\|V_{++}^{-1}V_{+-}\|\\
%         &+\|V_{--}^{-1}V_{--}\|\big)
%         \|V^{-1}\|\|M\|+\mathcal{O}(\epsilon^2),
%     \end{split}
%     \end{align}
%     for $\|F_{++}\|\|F_{--}^{-1}\|<\epsilon$ and $\epsilon$ as in Proposition~\ref{prop:errOWNSP}.
% \end{proposition}
% \begin{proof}
%     See Appendix~\ref{app:Proofs-P}.
% \end{proof}
% \begin{remark}
%     Proposition~\ref{prop:errCommP} shows that although $P_{N_\beta}$ and $M$ do not commute, the error introduced by treating them as if they do is small when $\|P-P_{N_\beta}\|$ is small.
% \end{remark}

\subsection{Approximate projection using OWNS-R}\label{subsec:OWNSR}

OWNS-R constructs approximations to $P$ with a reduced computational cost relative to OWNS-P. Definition~\ref{def:filter-r} presents the OWNS-R recursive filter, which is recast in matrix form in Proposition~\ref{prop:owns-r-implementation}. In contrast to OWNS-P, Proposition~\ref{prop:projMatOWNS-R} shows that the resulting matrix is not generally a projection matrix, while Proposition~\ref{prop:commMatOWNS-R} shows that it always commutes with $M$. Proposition~\ref{prop:OWNS-R-converge} provides a criterion for convergence, while Proposition~\ref{prop:minimalOWNS-R} provides a necessary and sufficient condition for convergent parameters to exist. While Zhu and Towne~\cite{Zhu_2021_OWNS-R} suggested that OWNS-R convergence can be guaranteed using the same recursion parameters as OWNS-P, we show in Proposition~\ref{prop:minimalOWNS-R-false} that this is not true. In addition, Proposition~\ref{prop:owns-r-repeat-blowup} shows that repeated applications of the OWNS-R matrix leads to unbounded growth or decay unless the filter is fully converged.

\begin{definition}[OWNS-R filter]\label{def:filter-r}
    Given the solution $\hat{\bm{\phi}}$, OWNS-R returns the filtered solution $\hat{\bm{\phi}}^{N_\beta}$ by solving
    \begin{subequations}
        \begin{align}
            \hat{\bm{\phi}}^0&=\frac{1}{h}\hat{\bm{\phi}},\\
            (M-i\beta_*^j)\hat{\bm{\phi}}^j
            &=
            (M-i\beta_-^j)\hat{\bm{\phi}}^{j-1},\quad j=1,\dots,N_\beta,
        \end{align}
        \label{eq:filter-r}
    \end{subequations}
         where $\{\beta_+^j,\beta_-^j\}_{j=1}^{N_\beta}$ are the recursion parameters and $\{\hat{\bm{\phi}}^j\}_{j=0}^{N_\beta}$ are the auxiliary variables, while $\{\beta^j_*\}_{j=1}^{N_\beta}$ and $h$ satisfy
        \begin{equation}
            h\prod_{j=1}^{N_\beta}(\alpha-\beta^j_*)
            =\prod_{j=1}^{N_\beta}(\alpha-\beta^j_-)
            +c\prod_{j=1}^{N_\beta}(\alpha-\beta^j_+),
            \label{eq:owns-r-polynomial}
        \end{equation}
    for freely-chosen $c\geq0$ (see Remark~\ref{rmk:owns-r-c} for more information on choosing $c$). Note that $\{\beta^j_*\}_{j=1}^{N_\beta}$ and $h$ must be computed numerically (e.g., via \texttt{roots} in MATLAB)~\cite{Zhu_2021_OWNS-R}.
\end{definition}

\begin{proposition}\label{prop:owns-r-implementation}
    The recursive filter~\eqref{eq:filter-r} can be recast in matrix form
    \begin{equation}
        P^{(R)}_{N_{\beta}}
        =V E_{N_\beta} V^{-1},\quad E_{N_\beta}=(1+cF)^{-1},
        \label{eq:matOWNSR}
    \end{equation}
    where
    \begin{equation}
        E_{N_\beta}^{(k)}
        =
        \frac{\prod_{j=1}^{N_\beta}(\alpha_k-\beta_-^j)}{\prod_{j=1}^{N_\beta}(\alpha_k-\beta_-^j) + c \prod_{j=1}^{N_\beta}(\alpha_k-\beta_+^j)},\quad k=1,\dots,N,
    \end{equation}
    for $F$ defined in~\eqref{def:matF}.
\end{proposition}
\begin{proof}
    See Zhu and Towne~\cite{Zhu_2021_OWNS-R}.
\end{proof}

\begin{remark}\label{rmk:owns-r-c}
    Zhu and Towne~\cite{Zhu_2021_OWNS-R} recommend setting $c\approx1$ since $c=0$ retains all modes, while $c\to\infty$ removes all modes. Therefore, we set $c=1$.
\end{remark}

\begin{proposition}\label{prop:projMatOWNS-R}
    $P_{N_\beta}^{(R)}$ is a projection matrix if and only if $E_{N_\beta}^{(k)}\in\{0,1\}$ for all $k=1,\dots,N$.
\end{proposition}
\begin{proof}
    We have $(P^{(R)}_{N_{\beta}})^2=VE^2_{N_\beta}V^{-1}$, so that $P_{N_\beta}^{(R)}$ is a projection matrix if and only if $E_{N_\beta,k}^2=E_{N_\beta,k}$ for $k=1,\dots,N$, which requires $E_{N_\beta,k}\in\{0,1\}$.
\end{proof}

\begin{proposition}\label{prop:commMatOWNS-R}
    $P_{N_\beta}^{(R)}$ commutes with $M$.
\end{proposition}
\begin{proof}
$P^{(R)}_{N_\beta}M=VE_{N_\beta}DV^{-1}=VDE_{N_\beta}V^{-1}=MP^{(R)}_{N_\beta}$ since $E_{N_\beta}$ and $D$ are diagonal matrices.
\end{proof}

\begin{proposition}\label{prop:OWNS-R-converge}
$P_{N_\beta}^{(R)}\to P$ if and only if $E_{N_\beta}^{(m)}\to 1$ for all $m\in i^{(+)}$ and $E_{N_\beta}^{(n)}\to0$ for all $n\in i^{(-)}$.
\end{proposition}
\begin{proof}
    See Zhu and Towne~\cite{Zhu_2021_OWNS-R}.
\end{proof}

\begin{proposition}\label{prop:minimalOWNS-R}
    Recursion parameters such that $P_{N_\beta}^{(R)}\to P$ exist if and only if $\alpha_m\neq\alpha_n$ for all $(m,n)\in i^{(+)}\times i^{(-)}$. In particular, take $N_\beta=\tilde{N}_++\tilde{N}_-$ with
    \begin{subequations}
    \begin{align}
        \beta_+^{j}&=\alpha_{\tilde{i}^{(+)}(j)},\quad
        \beta_-^{j}\neq\alpha_{\tilde{i}^{(+)}(j)},\quad
        j=1,\dots,\tilde{N}_+,\\
        \beta_-^{\tilde{N}_++j}&=\alpha_{\tilde{i}^{(-)}(j)},\quad
        \beta_+^{\tilde{N}_++j}\neq\alpha_{\tilde{i}^{(-)}(j)},\quad j=1,\dots,\tilde{N}_-.
    \end{align}
    \label{eq:OWNS-R-params}
    \end{subequations}
\end{proposition}
\begin{proof}
    If there exists $(\hat{m},\hat{n})\in i^{(+)}\times i^{(-)}$ such that $\alpha_{\hat{m}}=\alpha_{\hat{n}}$, then $E_{N_\beta}^{(\hat{m})}=E_{N_\beta}^{(\hat{n})}$, but convergence requires $E_{N_\beta}^{(\hat{m})}\to1$ and $E_{N_\beta}^{(\hat{n})}\to0$, which is not possible. If $\alpha_m\neq\alpha_n$ for $(m,n)\in i^{(+)}\times i^{(-)}$, then choose recursion parameters according to~\eqref{eq:OWNS-R-params} so that
    \begin{align*}
    E_{\tilde{N}_++\tilde{N}_-}^{(m)}
    &=\frac{\prod_{j=1}^{\tilde{N}_++\tilde{N}_-}(\alpha_m-\beta_-^j)}{\prod_{j=1}^{\tilde{N}_++\tilde{N}_-}(\alpha_m-\beta_-^j)+0}
    =1,\quad \forall m\in i^{(+)},\\
    E_{\tilde{N}_++\tilde{N}_-}^{(n)}
    &=\frac{0}{0+c\prod_{j=1}^{\tilde{N}_++\tilde{N}_-}(\alpha_n-\beta_+^j)}
    =0,\quad \forall n\in i^{(-)},
    \end{align*}
    where we have used that $\alpha_m \in \{\beta_+^j\}$, $\alpha_m \notin \{\beta_-^j\}$ for all $m \in i^{(+)}$ and $\alpha_n \in \{\beta_-^j\},$ $\alpha_n \notin \{\beta_+^j\}$ for all $n \in i^{(-)}$.    
\end{proof}

\begin{proposition}\label{prop:minimalOWNS-R-false}
    The recursion parameters that guarantee OWNS-P convergence (Proposition~\ref{prop:minimalOWNS-P}) do not guarantee OWNS-R convergence.
\end{proposition}
\begin{proof}
    If $\tilde{N}_+\leq \tilde{N}_-$, use~\eqref{eq:owns-p-params_plus}. Then there exists $n\in i^{(-)}$ such that $\alpha_n\notin\{\beta_-^j\}_{j=1}^{N_\beta}$ so that $E_{N_\beta}^{(n)}\neq0$ and $P_{N_\beta}^{(R)}\neq P$. If $\tilde{N}_-<\tilde{N}_+$, use~\eqref{eq:owns-p-params_minus}. Then there exists $m\in i^{(+)}$ such that $\alpha_m\notin\{\beta_+^j\}_{j=1}^{N_\beta}$ so that $E_{N_\beta}^{(m)}\neq1$ and $P_{N_\beta}^{(R)}\neq P$
\end{proof}

\begin{proposition}\label{prop:minimalOWNS-R-true}
    The recursion parameters that guarantee OWNS-R convergence (Proposition~\ref{prop:minimalOWNS-R}) also guarantee OWNS-P convergence.
\end{proposition}
\begin{proof}
Taking $N_\beta=N$ with parameters~\eqref{eq:OWNS-R-params} guarantees that $\alpha_m\in\{\beta_+^j\}_{j=1}^{N_\beta}$ for all $m\in i^{(+)}$ and $\alpha_n\in\{\beta_-^j\}_{j=1}^{N_\beta}$ for all $n\in i^{(-)}$, so that the convergence criterion~\eqref{eq:fConv} is satisfied (see Proposition \ref{prop:approxProjectConvergence}).
\end{proof}


\begin{proposition}\label{prop:errOWNSR}
    The error introduced by OWNS-R is bounded by
    \begin{equation}
        \|P-P_{N_\beta}^{(R)}\|\leq
        \max\big\{ |c|\|F_{++}\|, \|F_{--}^{-1}\| \big\}\|V\|\|V^{-1}\|
        +\mathcal{O}(\epsilon^2),
    \end{equation}
    where $\|F_{++}\|,\|F_{--}\|<\epsilon$ for small $\epsilon>0$ such that $\epsilon\ll1$.
\end{proposition}

\begin{proof}
    See Zhu and Towne~\cite{Zhu_2021_OWNS-R}.
\end{proof}

\begin{proposition}~\label{prop:greedy-r}
    If $\alpha_m\in\{\beta_+^j\}_{j=1}^{N_\beta}$ for any $m\in i^{(+)}$, then $P_{N_\beta}^{(R)}\bm{v}_m=\bm{v}_m$; if $\alpha_n\in\{\beta_-^j\}_{j=1}^{N_\beta}$ for any $n\in i^{(-)}$, then $P_{N_\beta}^{(R)}\bm{v}_n=0$.
\end{proposition}

\begin{proof}
    If $\alpha_m\in\{\beta_+^j\}_{j=1}^{N_\beta}$ for $m\in i^{(+)}$, then $E_{N_\beta}^{(m)}=1$ so that $P_{N_\beta}^{(R)}\bm{v}_{m}=\bm{v}_m$. If $\alpha_n\in\{\beta_-^j\}_{j=1}^{N_\beta}$ for $n\in i^{(-)}$, then $E_{N_\beta}^{(n)}=0$ so that $P_{N_\beta}^{(R)}\bm{v}_{n}=0$.
\end{proof}

\begin{remark}
Proposition~\ref{prop:errOWNSR} establishes that the error in introduced by OWNS-R scales with $\max\{ |c|\|F_{++}\|, \|F_{--}^{-1}\| \}$ for sufficiently small $\|F_{++}\|$ and $\|F_{--}\|$, while Proposition~\ref{prop:greedy-p} shows it is always possible to achieve zero error for any eigenvector by using its eigenvalue as a recursion parameter.
\label{rmk:error-compare}
\end{remark}

\begin{proposition}\label{prop:owns-r-repeat-blowup}
For any $k=1,\dots,N$, repeated application of $P_{N_\beta}^{(R)}$ causes unbounded growth of $\bm{v}_k$ when $|E_{N_\beta}^{(k)}|>1$ and decay to zero when $|E_{N_\beta}^{(k)}|<1$.
\end{proposition}
\begin{proof}
    Applying $P_{N_\beta}^{(R)}$ $n$ times yields $(P_{N_\beta}^{(R)})^n\bm{v}_k = V (E_{N_\beta})^n V^{-1}\bm{v}_k$. If $|E_{N_\beta}^{(k)}| < 1$, then mode $k$ will be removed ($\lim_{n\to\infty}|E_{N_\beta}^{(k)}|=0$), which is undesirable for right-going modes. In contrast, it will grow without bound if $|E_{N_\beta}^{(k)}| > 1$ ($\lim_{n\to\infty}|E_{N_\beta}^{(k)}|=\infty$), which is undesirable for both right- and left-going modes.
\end{proof}

\begin{remark}
Proposition~\ref{prop:projMatOWNS-R} shows that $P_{N_\beta}^{(R)}$ is not generally a projection matrix, while Proposition~\ref{prop:owns-r-repeat-blowup} shows that repeated application of $P_{N_\beta}^{(n)}$ introduces additional error unless $|E_{N_\beta}^{(n)}|<1$ for all $n\in i^{(-)}$, and $E_{N_\beta}^{(m)}=1$ for all $m\in i^{(+)}$, so that it should only be applied once. In contrast, OWNS-P does not have this limitation since $P_{N_\beta}$ is always a projection matrix by Proposition~\ref{prop:approxProjectProjectMat}.
\end{remark}

\begin{remark}~\label{rmk:rounding}
    In theory, the OWNS-R error decreases with increasing $N_\beta$ if both $\|F_{++}\|$ and $\|F_{--}^{-1}\|$ decrease. In practice, we observe that for sufficiently large $N_\beta$, rounding errors due to finite precision arithmetic prevent the OWNS-R filter from converging. We note that solving~\eqref{eq:filter-r} can introduce rounding errors for large $N_\beta$, while Appendix~\ref{app:Rounding} discusses how computing $\beta_j^*$ numerically (e.g., via \texttt{roots} in MATLAB) introduces rounding errors that increase as a function of $N_\beta$.
\end{remark}

\begin{remark}
    OWNS-R was developed to yield a lower computational cost than OWNS-P (and OWNS-O). We refer to Zhu and Towne~\cite{Zhu_2021_OWNS-R} for a more detailed discussion of computational cost of OWNS-P and OWNS-R.
\end{remark}


\subsection{Summary and comparison of the OWNS formulations}

The OWNS-P error scales with $\|F_{++}\|\|F_{--}^{-1}\|$, so that we have convergence with either $\|F_{--}^{-1}\|=0$ or $\|F_{++}\|=0$, while the OWNS-R error scales with $\max\{|c|\|F_{++}\|, \|F_{--}^{-1}\| \}$ so that we must instead have $\|F_{--}^{-1}\|=\|F_{++}\|=0$ for convergence. Thus, OWNS-R will generally require larger $N_\beta$. Although similar strategies can be used to pick recursion parameters for both OWNS-P and OWNS-R, Proposition~\ref{prop:minimalOWNS-R-false} shows that there exists parameter sets for which OWNS-P is converged while OWNS-R is not.

Whereas OWNS-P yields a projection matrix that generally does not commute with $M$, OWNS-R yields a matrix that commutes with $M$ but is not generally a projection matrix, so that OWNS-P and OWNS-R each lose one of the properties of $P$ (projection matrix or commutation with $M$). Although $P_{N_\beta}$ is a projection matrix, we will have $P_{N_\beta}\hat{\bm{\phi}}_{N_\beta}'
\neq\hat{\bm{\phi}}_{N_\beta}'$, where $\hat{\bm{\phi}}'_{N_\beta}$ is the solution to the OWNS-P approximation to~\eqref{eq:lowns1}, since $P_{N_\beta}M=M P_{N_\beta}$ (unless the approximation is fully converged). Although $P_{N_\beta}^{(R)}$ commutes with $M$, we will have $P_{N_\beta}^{(R)}\hat{\bm{\phi}}_{N_\beta}^{(R)\prime}
\neq\hat{\bm{\phi}}_{N_\beta}^{(R)\prime}$, where $\hat{\bm{\phi}}^{(R)\prime}_{N_\beta}$ is the solution to the OWNS-R approximation to~\eqref{eq:lowns1}, since $P_{N_\beta}^{(R)}\neq P_{N_\beta}^{(R)}$ (unless the approximation is fully converged). Moreover, by Proposition~\ref{prop:owns-r-repeat-blowup}, repeated applications of $P_{N_\beta}^{(R)}$ should be avoided to prevent non-physical solution blow-up or decay.


\section{Greedy algorithm for recursion parameter selection}\label{sec:greedy}

If $\alpha_m \neq \alpha_n$ for all $(m,n) \in i^{(+)}\times i^{(-)}$, then Propositions~\ref{prop:minimalOWNS-P} and~\ref{prop:minimalOWNS-R} identify recursion parameter sets that ensure convergence. However, these sets are large, leading to large linear systems of equations~\eqref{eq:filter-p} for OWNS-P, while OWNS-R suffers from rounding errors in computing $\beta_{j}^{*}$, which renders its filter~\eqref{eq:filter-r} inaccurate. To overcome this challenge, previous work has used \textit{heuristic} estimates of key eigenvalues to obtain stable and accurate approximation with $N_\beta\ll N$. However, this requires problem-specific trial-and-error parameter tuning and leads to slow convergence. Therefore, we propose an optimization procedure to automatically choose recursion parameters that ensure rapid filter convergence.

We requires the following ingredients: (i) an objective function to assess recursion parameter performance, (ii) candidate recursion parameters, and (iii) an optimization routine. We will use the error bounds from Propositions~\ref{prop:errOWNSP} and~\ref{prop:errOWNSR} to obtain objective functions, which we will minimize for fixed $N_\beta\ll N$ using a greedy algorithm to select a sparse subset of the eigenvalues of $M$ as our recursion parameter set.


\subsection{Objective function}

We define the recursion parameter set $\Xi_{N_{\beta}}\equiv\{\beta_+^j,\beta_-^j\}_{j=1}^{N_\beta}$ and the functions
\begin{subequations}
\begin{equation}
    \hat{\mathcal{J}}_+(\alpha;\Xi_{N_{\beta}})\equiv
    \prod_{j=1}^{N_{\beta}}
    \frac{|\alpha-\beta_{+}^{j}|}{|\alpha-\beta_-^j|},\quad
    \hat{\mathcal{J}}_-(\alpha;\Xi_{N_{\beta}})\equiv
    \prod_{j=1}^{N_{\beta}}
    \frac{|\alpha-\beta_{-}^{j}|}{|\alpha-\beta_+^j|},
\end{equation}
so that by Proposition~\ref{prop:errOWNSP} the OWNS-P error scales as
\begin{equation}
\mathcal{J}(\Xi_{N_{\beta}})
\equiv
\max_{m\in i^{(+)}}
\hat{\mathcal{J}}_+(\alpha_m;\Xi_{N_{\beta}})
\max_{m\in i^{(-)}}
\hat{\mathcal{J}}_-(\alpha_n;\Xi_{N_{\beta}})
=\|F_{++}\|\|F_{--}^{-1}\|,
\label{eq:objFncRecursions}
\end{equation}
while by Proposition~\ref{prop:errOWNSR} the OWNS-R error scales as
\begin{align}
\begin{split}
\mathcal{J}^{(R)}(\Xi_{N_{\beta}})
&\equiv
\max\big\{
\max_{m\in i^{(+)}}\hat{\mathcal{J}}_+(\alpha_m;\Xi_{N_{\beta}}),
\max_{n\in i^{(-)}}\hat{\mathcal{J}}_-(\alpha_n;\Xi_{N_{\beta}})
\big\}\\
&=
\max\big\{\|F_{++}\|,\|F_{--}^{-1}\|\big\}.
\end{split}
\label{eq:objFncRecursionsR}
\end{align}
For brevity, we will use $\mathcal{J}$ to refer to both~\eqref{eq:objFncRecursions} and~\eqref{eq:objFncRecursions}.

\end{subequations}


\subsection{Candidate recursion parameters}

Propositions~\ref{prop:minimalOWNS-P} and~\ref{prop:minimalOWNS-R} use the eigenvalues of $M$ to achieve zero filter error, but this requires $N_\beta=\min\{\tilde{N}_+,\tilde{N}_-\}$ and $N_\beta=\tilde{N}_++\tilde{N}_-$ for OWNS-P and OWNS-R, respectively, while we want $N_\beta\ll N$. Therefore, we seek a subset of the eigenvalues that minimizes our objective functions for $N_\beta\ll N$.


\subsection{Greedy recursion parameter selection}

Given the eigenvalues $\{\alpha_k\}_{k=1}^{N}$, we wish to solve the optimization problem
\begin{equation}
    \min_{
    \hat{i}^{(+)}\subset i^{(+)},
    \hat{i}^{(-)}\subset i^{(-)}
    }
    \mathcal{J}(\{
    \alpha_{\hat{i}^{(+)}(j)},
    \alpha_{\hat{i}^{(-)}(j)}
    \}_{j=1}^{N_\beta}),
\end{equation}
for the subsets
\[
\hat{i}^{(+)}=\{\hat{i}^{(+)}(1),\dots,\hat{i}^{(+)}(N_\beta)\}\subset i^{(+)},\quad
\hat{i}^{(-)}=\{\hat{i}^{(-)}(1),\dots,\hat{i}^{(-)}(N_\beta)\}\subset i^{(-)}.
\]
This problem is well-posed because the objective function is continuous (it is a quotient of polynomials with a non-zero denominator), but identifying the globally optimal set is computationally intractable because we must consider $[N_+! / (N_+-N_\beta)!][N_-! / N_\beta! / (N_--N_\beta)!]/N_\beta!/N_\beta!$
combinations. Therefore, we use the greedy algorithm outlined in Algorithm~\ref{alg:greedyRecursion} to append the left- and right-going eigenvalues that lead to the largest decrease of the objective function at each iteration.

\begin{remark}\label{rmk:tracking}
    Previous work on OWNS has combined heuristic parameter selection with mode-tracking, where important discrete modes (e.g., Kelvin-Helmholtz in free-shear flows or Mack's second mode in high-speed boundary layers) were added to the recursion parameter set to improve approximation accuracy~\cite{Towne_2015_OWNS-O,Towne_2022_OWNS-P,Zhu_2021_OWNS-R}. While previous methods manually identified eigenvalues to track, the present work proposes an automatic approach.
\end{remark}
\begin{remark}
    The greedy algorithm can be initialized with user-specified recursion parameters, but we have found that it is better to initialize the greedy algorithm with many different randomly chosen pairs of left- and right-going eigenvalues, and to choose the parameter set with the smallest $\mathcal{J}$.
\end{remark}

\begin{algorithm}
\caption{Greedy recursion parameter selection}\label{alg:greedyRecursion}
\begin{algorithmic}
\STATE $\{\beta_+^{1},\beta_-^{1}\} \gets \{\alpha_m,\alpha_n\}$ for randomly chosen $m\in i^{(+)}$ and $n\in i^{(-)}$
\STATE $N_{\mathrm{greedy}} \gets 1$
\STATE $\Xi_{N_{\mathrm{greedy}}} \gets \{\beta_+^1,\beta_-^1\}$
\WHILE{$N_{\mathrm{greedy}} < N_\beta$}
\STATE $N_{\mathrm{greedy}}\gets
N_{\mathrm{greedy}} + 1$
\STATE $\tilde{m} \gets \arg\max_{m\in i^{(+)}}
\hat{\mathcal{J}}_+(\alpha_m;\Xi_{N_{\mathrm{greedy}}})$
\STATE $\tilde{n} \gets \arg\max_{n\in i^{(-)}}
\hat{\mathcal{J}}_-(\alpha_n;\Xi_{N_{\mathrm{greedy}}})$
\STATE $\{
\beta_+^{N_{\mathrm{greedy}}},
\beta_-^{N_{\mathrm{greedy}}}
\}\gets
\{\alpha_{\tilde{m}},\alpha_{\tilde{n}}\}$
\STATE $\Xi_{N_{\mathrm{greedy}}} \gets \{\beta_{+}^j,\beta_{-}^j\}_{j=1}^{N_{\mathrm{greedy}}}$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\subsection{Numerical stability of the recursive filter}

OWNS solves equations of the form $(M-i\beta_-^j)\hat{\bm{\phi}}^{j+1}=(M-i\beta_+^j)\hat{\bm{\phi}}^{j}$ for $j=0,\dots,N_\beta-1$. If the exact solution is given by $\hat{\bm{\varphi}}^{j+1}$, then the rounding error introduced by solving the linear system is bounded by
\[
    \frac{\|\hat{\bm{\phi}}^{j+1}-\hat{\bm{\varphi}}^{j+1}\|}{\|\hat{\bm{\varphi}}^{j+1}\|}
    \leq
    \kappa(M-i\beta_-^jI)\frac{\|M-i\beta_-^jI\|\|\hat{\bm{\phi}}^{j+1}-\hat{\bm{\phi}}^{j}\|+|\beta_+^j-\beta_-^j|\|\hat{\bm{\phi}}^j\|}{\|(M-i\beta_+I)\hat{\bm{\varphi}}^j\|}.
\]
If $\beta_+^j=\beta_-^j$, then $\|\hat{\bm{\phi}}^{j+1}-\hat{\bm{\phi}}^{j}\|=\mathcal{O}(\epsilon_{\mathrm{mach}})$, where $\epsilon_{\mathrm{mach}}$ is machine epsilon, and the error is $\mathcal{O}(\epsilon_{\mathrm{mach}}\kappa(M-i\beta_-^jI))$. However, this error grows as $|\beta_+^j-\beta_-^j|$ grows, and accumulates for $j=0,\dots,N_\beta-1$. Therefore, it is preferable for $|\beta_-^j-\beta_+^j|$ to be small, so we re-order the recursion parameters such that $|\beta_+^{j-1}|<|\beta_+^{j}|$ and $|\beta_-^{j-1}|<|\beta_-^{j}|$ for $j=1,\dots,N_\beta-1$. The error bounds do not depend on the order of the recursion parameters, but is critical to sort them by magnitude when implementing OWNS on a computer.


\subsection{Greedy algorithm and spatial marching}

For systems where $M$ varies slowly in $x$, the recursion parameters computed at the inlet can be reused at subsequent stations. However, they generally need to be updated to keep the filter error low, which can be done by \textit{tracking} them as they evolve downstream (Algorithm~\ref{alg:greedyTracking}). When the number of left- and right-going characteristics changes, such as supersonic boundary-layer flows when the number of subsonic and supersonic grid points changes, it can be advantageous (although not necessary) to update the parameters using Algorithm~\ref{alg:greedyRecursion}.

Consider the recursion parameters, $\Xi_{N_\beta}^{(i-1)}$, at station $i-1$, which are eigenvalues of $M^{(i-1)}$. They can be used as an input to a sparse eigenvalues solver, such as MATLAB's \texttt{eigs} function, to compute $\Xi_{N_\beta}^{(i)}$. For large, sparse $M$, this is much less costly than solving for the complete eigenspectrum.


\begin{algorithm}
\caption{Recursion parameter tracking}\label{alg:greedyTracking}
\begin{algorithmic}
\STATE Compute $N_+^{(1)}$ and $N_-^{(1)}$
\STATE Choose $\Xi_{N_\beta}^{(1)}$ using Algorithm~\ref{alg:greedyRecursion}
\FOR{$i=2,\dots,N_x$}
\STATE Compute $N_+^{(i)}$ and $N_-^{(i)}$
\IF{$N_+^{(i)}=N_+^{(i-1)}$}
    \FOR{$j=0,\dots,N_\beta$}
    \STATE $\{
    \tilde{\beta}_+,\tilde{\beta}_-\}
    \gets
    \{\beta_+^{j},\beta_-^{j}\}^{(i-1)}
    $
    \STATE Compute $\beta_+^j$ and $\beta_-^j$ by finding eigenvalue of $M^{(i)}$ closest to $\tilde{\beta}_+$ and $\tilde{\beta}_-$
    \STATE $\{\beta_+^{j},\beta_-^{j}\}\gets\{\beta_+,\beta_-\}$
    \ENDFOR
    \STATE $\Xi_{N_{\beta}}^{(i)} \gets \{\beta_{+}^j,\beta_{-}^j\}_{j=1}^{N_\beta}$
\ELSE
\STATE Compute $\Xi_{N_\beta}^{(i)}$ using Algorithm~\ref{alg:greedyRecursion} with the eigenvalues of $M^{(i)}$
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}


\subsection{Cost trade-off}

Despite the added cost that the greedy algorithm incurs when computing the eigenvalues of $M$, it can still lead to a net decrease in computational cost, relative to heuristic selection, since it decreases the number of recursion parameters required to achieve a desired error tolerance. Moreover, the full eigenspectrum is required only at a small number of stations, potentially just the domain inlet, and the selected eigenvalues can be efficiently updated by \textit{tracking} using a sparse eigenvalues solver. We also note that in optimization tasks, such as the resolvent analysis by Towne et al.~\cite{Towne_2022_OWNS-P}, the initial cost of computing recursion parameters can be amortized over multiple iterations of the optimization routine.

\begin{remark}
    This work focuses on flows with only one inhomogeneous direction, but computing eigenvalues for flows with two or more inhomogeneous directions may not be feasible, making heuristic selection the preferred method for these flows.
\end{remark}


\section{Extension to the Navier-Stokes equations}\label{sec:NSE}

While the Navier-Stokes equations are not hyperbolic, if we neglect the viscous terms, we obtain the Euler equations, which are hyperbolic. To apply the one-way marching routine to the Navier-Stokes equations, the streamwise viscous terms are either neglected or treated as a forcing function~\cite{Towne_2022_OWNS-P,Kamal_2020_HOWNS,Sleeman_2025_NOWNS_AIAAJ}; we choose to treat them as a forcing function.


\subsection{Governing equations}

The non-dimensional compressible Navier-Stokes equations for an ideal gas can be written as
\begin{subequations}
\begin{align}
    \frac{\partial\rho}{\partial t}+\nabla\cdot(\rho\bm{u})
    &=0,\\
    \rho\frac{D\bm{u}}{D t}+\nabla p 
    &= \frac{1}{Re}\nabla\cdot\tau,\\
    \rho c_p\frac{D T}{D t}-\frac{Dp}{Dt}
    &=\frac{1}{Pr Re}\nabla\cdot\big(k(T)\nabla T\big)+\frac{1}{Re}\tau :\nabla\bm{u}.
\end{align}
for the stress tensor
\begin{equation}
    \tau = \mu(T) \big(\nabla\bm{u}+(\nabla\bm{u})^T\big)-\big(\frac{2}{3}\mu(T)-\kappa(T)\big)(\nabla\cdot\bm{u})I,
\end{equation}
\label{eq:NSE}
\end{subequations}
where $\rho$ is the density, $\bm{u}$ is the velocity, $p$ is the pressure, $T$ is the temperature, $c_p$ is the heat capacity at constant pressure, $\mu$ is the dynamic viscosity, and $\kappa$ is the bulk viscosity. We take the bulk viscosity to be zero ($\kappa = 0$) and introduce the Reynolds number, $Re = \rho_\infty^*a_{\infty}^*\delta_0^* / \mu_\infty^*$, Prandtl number, $Pr = c_{p,\infty}^*\mu_\infty^* / \kappa_\infty^*$, and Blasius length scale, $\delta_0^*=\sqrt{\mu_\infty^* x_0^* / \rho_\infty^*u_\infty^*}$.
Here, $(\cdot)_\infty$ and $(\cdot)^*$ denote freestream and dimensional quantities, respectively, while $\mu$ is the dynamic viscosity, $a$ is the speed of sound, $u$ is the streamwise velocity, and $x_0$ denotes the inlet boundary. We assume that the dynamic viscosity is a function of temperature according to Sutherland's law, while the Prandtl number and specific heat capacity are constant. We additionally introduce the dimensionless frequency $F = \omega^*\mu_\infty^* /\rho_\infty^*u_\infty^{*2}$, the dimensionless wavenumber $b = \beta^*\mu_\infty^* / \rho_\infty^*u_\infty^*$ and the Reynolds number (based on the streamwise coordinate) $Re_x = u_\infty^*x^*/\rho_\infty^*\mu_\infty^*$, where $\omega$ is the temporal frequency and $\beta$ is spanwise wavenumber.


\subsection{Linear OWNS}

Linear stability calculations evolve infinitesimal disturbances, $\bm{q}'$, to a steady equilibrium solution, $\bar{\bm{q}}$, where $\bm{q}=(\rho,\bm{u},T)$ denotes the vector of primitive variables. Neglecting streamwise viscous terms, the linear OWNS equations can be written
\begin{equation}
    A(\bar{\bm{q}})\frac{\partial\bm{q}'}{\partial x} = L(\bar{\bm{q}})\bm{q}',\quad \bm{q}'(x,y,z,t)=\hat{\bm{q}}(x,y)e^{i(\beta z - \omega t)}
\end{equation}
where we have linearized about the steady equilibrium solution and discretized in the wall-normal direction using a 4th-order central finite difference scheme.


\subsection{Nonlinear OWNS}

If the disturbances have finite amplitude, then disturbances of different frequency and spanwise wavenumber will interact nonlinearly with each other, so that we must perform a nonlinear calculation. The nonlinear OWNS equations can be written
\begin{subequations}
\begin{equation}
    A(\bar{\bm{q}})\frac{\partial\hat{\bm{q}}_{mn}}{\partial x}
    = \hat{L}_{mn}(\bar{\bm{q}})\hat{\bm{q}}_{mn}+\hat{\bm{F}}_{mn}(\bm{q}')+\hat{\bm{f}}_{mn},
\end{equation}
for $m=-M,\dots,M$ and $n=-N,\dots,N$, where
\begin{align}
    \bm{q}'(x,y,z,t)
    &=\sum_{m,=-M}^{M}\sum_{m,=-N}^{N}\hat{\bm{q}}_{mn}(x,y)e^{i(n\beta z-m\omega t)},\\
    L(\bar{\bm{q}})\bm{q}'
    &=\sum_{m=-M}^{M}\sum_{n=-N}^{N}\hat{L}_{mn}\hat{\bm{q}}_{mn}e^{i(n\beta z-m\omega t)},\\
    F(\bm{q}')
    &=\sum_{m=-M}^{M}\sum_{n=-N}^{N}\hat{F}_{mn}(\bm{q}')e^{i(n\beta z-m\omega t)}.
\end{align}
\end{subequations}
Note that we truncate the Fourier series at $M$ temporal and $N$ spanwise Fourier modes, while details on NOWNS are presented by Sleeman et al.~\cite{Sleeman_2025_NOWNS_AIAAJ}.


\section{Demonstration of the greedy algorithm for a single station}\label{sec:greedySingle}

We demonstrate the greedy algorithm for the oblique breakdown case studied by Joslin et al.~\cite{Joslin_1993_DNS} for a low-speed isothermal flat plate boundary-layer flow with disturbance frequency and wavenumber are $F = 86 \times 10^{-6}$ and $b = 0.222 \times 10^{-3}$, respectively, for the wall-normal domain $y\in[0,60]$ with $N_y=100$ at $Re_x=2.74\times10^5$. We find that greedy parameter selection yields faster error convergence for both OWNS-P and OWNS-R. We also find that OWNS-R with heuristic parameter selection does not properly remove all left-going modes, which is related to the rounding errors due to finite prediction arithmetic (see Remark~\ref{rmk:rounding}).


\subsection{Error convergence}

The projection error scales with the objective function $\mathcal{J}$, while we can measure it by considering a solution $\hat{\bm{\phi}}=V_{+}\hat{\bm{\psi}}_++V_{-}\hat{\bm{\psi}}_-$, where $P\hat{\bm{\phi}}=V_{+}\hat{\bm{\psi}}_+$ by definition. We choose random coefficients $\hat{\bm{\psi}}$, or coefficients associated with the TS wave, and compute the relative errors
\begin{subequations}
\begin{align}
    \mathrm{error}(\hat{\bm{\phi}})&
    =\frac{\|\hat{\bm{\phi}}'-P_{N_\beta}\hat{\bm{\phi}}\|}{\|\hat{\bm{\phi}}'\|}
    =\frac{\|V_+\bm{\psi}_+-P_{N_\beta}[V_+\bm{\psi}_++V_-\bm{\psi}_-]\|}{\|V_+\bm{\psi}_+\|},\label{eq:errPhi}\\
    \mathrm{error}(\hat{\bm{\phi}}_{\mathrm{TS}})&=
    \frac{\|\hat{\bm{\phi}}_{\mathrm{TS}}-P_{N_\beta}\hat{\bm{\phi}}_{\mathrm{TS}}\|}{\|\hat{\bm{\phi}}_{\mathrm{TS}}\|}
    =
    \frac{\|\hat{\bm{\phi}}_{\mathrm{TS}}-P_{N_\beta}\hat{\bm{\phi}}_{\mathrm{TS}}\|}{\|\hat{\bm{\phi}}_{\mathrm{TS}}\|}\label{eq:errPhiTS}.
\end{align}
\end{subequations}
Figures~\ref{fig:oblique-err-p} and~\ref{fig:oblique-err-r} plot $\mathcal{J}$ against the error~\eqref{eq:errPhi} for greedy and heuristic parameter selection for OWNS-P and OWNS-R, respectively, showing that the OWNS-P error scales with $\mathcal{J}$, while the OWNS-R error scales with $\mathcal{J}^{(R)}$ for small $N_\beta$. Due to rounding errors when computing $\beta_*^j$, the OWNS-R error increases for large values of $N_\beta$, even though $\mathcal{J}^{(R)}$ continues to decrease, as discussed in Remark~\ref{rmk:rounding}. We also see that greedy selection leads to faster convergence than heuristic selection for both OWNS-P and OWNS-R. Figure~\ref{fig:oblique-err-p} shows that greedy OWNS-P achieves machine zero error at $N_\beta=24$, while heuristic OWNS-P converges far more slowly and does not achieve machine zero error, even for much larger $N_\beta$. Greedy OWNS-R approaches machine zero error for $N_\beta\approx 40$, after which point the error increases due to rounding errors. We further see in Figure~\ref{fig:oblique-err-r} that OWNS-R with heuristic selection leads to large errors, which decrease slowly until $N_\beta\approx 58$, after which it also increases due to rounding errors.

Figures~\ref{fig:oblique-err-TS-p} and~\ref{fig:oblique-err-TS-r} show that greedy selection yields significantly less error in the TS wave than heuristic selection. The greedy algorithm selects the eigenvalue associated with the TS wave as a recursion parameter, as shown in Figure~\ref{fig:param-spectrum-greedy}, so by Propositions~\ref{prop:greedy-p} and~\ref{prop:greedy-r}, the filter error is zero. Figure~\ref{fig:param-spectrum-heuristic} shows the heuristic parameters for $N_\beta=20$. We see that although heuristic selection attempts to place its parameters near the TS wave and along the continuous branches, it does so less efficiently than the greedy algorithm. Although it accurately targets the left-going acoustic waves along the imaginary axis for $\mathcal{I}(\alpha)<0$, it distributes the parameters for the right-going acoustic waves (near the imaginary axis for $\mathcal{I}(\alpha)>0$) and the vortical waves (along the axis at $\pi/4$ radians for $\mathcal{I}(\alpha)>0$) less accurately. Moreover, it places parameters along the imaginary axis in the left half-plane, where there are no eigenvalues. Thus, the greedy parameters suggest avenues for improving the heuristic selection procedure.

\begin{remark}
    In Section~\ref{subsec:OWNSR}, our numerical analysis showed that OWNS-P will converge faster than OWNS-R, which is what we observe empirically in Figure~\ref{fig:greedy-err-converge}.
\end{remark}

% \begin{figure}
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/Oblique_Obj_P.pdf}
%         \caption{OWNS-P}
%         \label{fig:oblique-obj-p}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/Oblique_Obj_R.pdf}
%         \caption{OWNS-R}
%         \label{fig:oblique-obj-r}
%     \end{subfigure}
%     \caption{Objective function convergence for low-speed oblique breakdown.}
%     \label{fig:greedy-converge}
% \end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/Oblique_Err_P_bw.pdf}
        \caption{OWNS-P}\label{fig:oblique-err-p}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/Oblique_Err_R_bw.pdf}
        \caption{OWNS-R}\label{fig:oblique-err-r}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/Oblique_Err_TS_P_bw.pdf}
        \caption{OWNS-P, TS wave only}\label{fig:oblique-err-TS-p}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/Oblique_Err_TS_R_bw.pdf}
        \caption{OWNS-R, TS wave only}\label{fig:oblique-err-TS-r}
    \end{subfigure}\\
    \caption{Error convergence for low-speed oblique breakdown, where $\Xi_{N_\beta}^G$ and $\Xi_{N_\beta}^H$ denote the greedy and heuristic recursion parameter sets, respectively.}
    \label{fig:greedy-err-converge}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/Oblique_Params_Heuristic_bw.pdf}
        \caption{Heuristic}
        \label{fig:param-spectrum-heuristic}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/Oblique_Params_Greedy_bw.pdf}
        \caption{Greedy}
        \label{fig:param-spectrum-greedy}
    \end{subfigure}
    \caption{Recursion parameters plotted against spectrum for heuristic and greedy recursion parameter selection with $N_\beta = 20$.}
    \label{fig:param-spectrum}
\end{figure}

% Plotting $\hat{\mathcal{J}}_+$ and Figure~\ref{fig:oblique-obj-r-pm} shows that although $\hat{\mathcal{J}}_-$ decreases quickly for heuristic selection, $\hat{\mathcal{J}}_+$ decreases slowly. Since the OWNS-R objective function depends on the maximum value of these two functions, it decreases more slowly than the OWNS-P function. 
% \begin{figure}
%     \centering
%     \includegraphics[width=0.48\linewidth]{figures/Oblique_Obj_R_pm.pdf}
%     \caption{Convergence of the objective function, split into left- and right-going components, for greedy and heuristic parameter selection.}
%     \label{fig:oblique-obj-r-pm}
% \end{figure}


\subsection{Stability of the march}\label{subsec:stability}

All left-going modes must be removed for the march to be stable. Figure~\ref{fig:Oblique_OWNS_P_Briggs} shows that heuristic OWNS-P removes all left-going waves from $M$, while Figure~\ref{fig:Oblique_OWNS_R_Briggs} demonstrates that heuristic OWNS-R does not. For $\eta=0$, we observe that the OWNS-R spectrum, $P_{N_\beta}^{(R)}M$, has many eigenvalues with $\mathcal{I}(\alpha)<0$, and taking $\eta=1000$ allows us to verify that these are left-going modes according to Briggs' criterion. This demonstrates that there exists parameter sets for which OWNS-P is stable but OWNS-R is unstable. Repeating this analysis with greedy selection yields stable marches for both OWNS-P and OWNS-R.

% \begin{remark}
% Zhu and Towne~\cite{Zhu_2021_OWNS-R} noted that OWNS-P and OWNS-R approximate the $V$ and $E$, respectively. Therefore, we expect OWNS-P to better remove left-going modes since it makes no approximations to $E$, which is consistent with the above observations.
% \end{remark}

\begin{remark}
Figures 3 and 6 from Zhu and Towne~\cite{Zhu_2021_OWNS-R} show that the OWNS-R operator removes all left-going modes for the dipole and jet test cases, while figure 10 suggests it still supports left-going modes for their supersonic boundary-layer case. Thus, our observations in Figure~\ref{fig:Oblique_OWNS_R_Briggs} are consistent with those of Zhu and Towne~\cite{Zhu_2021_OWNS-R} for their supersonic boundary-layer flow case.
\end{remark}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/Oblique_OWNS_P_Briggs_bw.pdf}
        \caption{Heuristic OWNS-P, $N_\beta=15$}\label{fig:Oblique_OWNS_P_Briggs}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/Oblique_OWNS_R_Briggs_bw.pdf}
        \caption{Heuristic OWNS-R, $N_\beta=52$}\label{fig:Oblique_OWNS_R_Briggs}
    \end{subfigure}\\
    \caption{For the march to be stable, the OWNS spectrum ($P_{N_\beta}M$) must not support any left-going modes. According to Briggs' criterion, a mode is left-going if $\mathcal{I}(\alpha)<0$ as $\eta\to\infty$. Figure~\ref{fig:Oblique_OWNS_P_Briggs} shows that heuristic OWNS-P does not have $\mathcal{I}(\alpha)<0$ for $\eta=1000$, while~\ref{fig:Oblique_OWNS_R_Briggs} shows that heuristic OWNS-R does, so that the heuristic OWNS-R march is unstable.}
    \label{fig:greedy-stability}
\end{figure}


% \begin{figure}
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/Bertolotti_OWNS_P_Briggs_03.pdf}
%         \caption{OWNS-P, $N_\beta=3$}\label{fig:Bertolotti_OWNS_P_Briggs_03}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/Bertolotti_OWNS_P_Briggs_06.pdf}
%         \caption{OWNS-R, $N_\beta=6$}\label{fig:Bertolotti_OWNS_P_Briggs_06}
%     \end{subfigure}\\
%     \caption{Comparison of OWNS-P spectrum using heuristic parameter selection for $\eta=0$ and $\eta=1000$. According to Briggs' criterion, $\mathcal{I}(\alpha)<0$ for large values of $\eta$ indicates a right-going mode. For $\eta=0$, we wish to have $\mathcal{I}(\alpha)\geq0$ for the march to be stable. We achieve a stable march using OWNS-P and with both $N_\beta=3$ and $N_\beta=6$, because there are no left-going modes, according to Briggs' criterion.}
%     \label{fig:Bertolotti_OWNS_P_Briggs_compare}
% \end{figure}


%\subsubsection{Accuracy of the right-going eigenvalues}

%To further assess the performance of the recursion parameters, we plot the spectrum of $P_{N_\beta}M$ against that of $M$ for heuristic and greedy parameter selection in figure~\ref{fig:OWNS-spectrum-compare}. The greedy parameter selection properly tracks right-going modes for large values of $\mathcal{I}(\alpha)>0$, while heuristic parameter selection does not. Similarly, the greedy parameter selection properly tracks right-going vortical modes for $\mathcal{R}(\alpha)\in[0.042,0.044]$ and $\mathcal{I}(\alpha)\in(0,0.04)$, while heuristic parameter selection does not. However, most importantly, both greedy and heuristic parameter remove left-going waves and properly evolve track the TS wave, which explains why heuristic parameter selection works well in practice for OWNS-P. We make a similar comparison for OWNS-R in figure~\ref{fig:OWNS-R-spectrum-compare}, where we observe that the heuristic approach fares far worse for OWNS-R than OWNS-P, while the greedy approach works reasonably well. We note that the greedy approach for OWNS-R requires far more recursion parmaeters for accuracy and stability compared to OWNS-P.

% \begin{figure}
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/Bertolotti_OWNS_Spectrum_Heuristic.pdf}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/Bertolotti_OWNS_Spectrum_Greedy.pdf}
%     \end{subfigure}\\
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/Bertolotti_OWNS_Spectrum_Zoom_Heuristic.pdf}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/Bertolotti_OWNS_Spectrum_Zoom_Greedy.pdf}
%     \end{subfigure}
%     \caption{Comparison of the spectrum of $M$ against $P_{N_\beta}M$ for heuristic and greedy parameter selection.}
%     \label{fig:OWNS-spectrum-compare}
% \end{figure}

% \begin{figure}
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/Bertolotti_OWNS_R_Spectrum_Heuristic.pdf}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/Bertolotti_OWNS_R_Spectrum_Greedy.pdf}
%     \end{subfigure}\\
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/Bertolotti_OWNS_R_Spectrum_Zoom_Heuristic.pdf}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/Bertolotti_OWNS_R_Spectrum_Zoom_Greedy.pdf}
%     \end{subfigure}
%     \caption{Comparison of the spectrum of $M$ against $P_{N_\beta}^{(R)}M$ for heuristic and greedy parameter selection.}
%     \label{fig:OWNS-R-spectrum-compare}
% \end{figure}


% \subsection{Comparison for different flows}

% We expand our discussion to include a 3D low-speed boundary-layer flow and a 2D high-speed boundary-layer flow.

% \subsubsection{3D low-speed boundary-layer flow}~\label{sec:joslin}

% We investigate the oblique-wave case studied by Joslin et al.~\cite{Joslin_1993_DNS} for a low-speed isothermal flat plate boundary-layer flow with disturbance frequency and wavenumber are $F = 86 \times 10^{-6}$ and $b = 0.222 \times 10^{-3}$, respectively, for the wall-normal domain $y\in[0,60]$ with $N_y=100$ at $Re_x=2.74\times10^5$. Figure~\ref{fig:oblique-err} shows that greedy selection yields rapid convergence of the objective function and projection error. Moreover, OWNS-P achieves machine-zero error with $N_\beta=24$, while OWNS-R does not, due to rounding errors in computing $\beta_*^j$. We further note that the 3D case converges with fewer recursion parameters because the left- and right-going branches are more widely separated than in the 2D case, which makes it easier to choose recursion parameters that keep both $\|F_{++}\|$ and $\|F_{--}^{-1}\|$ small, as noted by Towne and Colonius~\cite{Towne_2015_OWNS-O}.


% \begin{figure}
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/Oblique_Err_Obj_P.pdf}
%         \caption{OWNS-P}\label{fig:oblique-err-p}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/Oblique_Err_Obj_R.pdf}
%         \caption{OWNS-R}\label{fig:oblique-err-r}
%     \end{subfigure}
%     \caption{Error convergence as a function of $N_\beta$ for OWNS-P and OWNS-R for the low-speed oblique wave case.}
%     \label{fig:oblique-err}
% \end{figure}


% \subsubsection{2D high-speed boundary-layer flow}~\label{sec:zhong}

% We consider the Mach 4.5 boundary-layer flow over an adiabatic flat plate studied by Ma and Zhong~\cite{Ma_2003_DNS_1}, which was used by Zhu and Towne~\cite{Zhu_2021_OWNS-R} as a validation case for the OWNS-R formulation. The flow conditions are $M_\infty = 4.5$, $T_\infty^*=65.15$ K, $p_\infty^*=728.44$ Pa, $Pr=0.72$, and unit Reynolds number $Re_\infty^* = \rho_\infty^* U_\infty^* / \mu_\infty^*=7.2\times10^6 / $ m. We compare the greedy and heuristic approaches at $Re_x=5.625\times10^3$, and figure~\ref{fig:zhong-err} shows the convergence of the error as a function of $N_\beta$ for OWNS-P and OWNS-R. We again observe that OWNS-P achieves machine zero error, while OWNS-R does not. For OWNS-R only, we have excluded right-going eigenvalues with $\alpha_i>100$ from the greedy selection procedure, which improves the accuracy of the approximation, since these modes decay rapidly. We also note that heuristic selection is better at evolving Mack's second mode (MM), which is the dominant instability for high-speed boundary-layer flows, as the they have been clustered near it for better accuracy~\cite{Kamal_2023_Thesis}.

% \begin{figure}
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/Zhong-OWNS-P.pdf}
%         \caption{OWNS-P}
%         \label{fig:zhong-err-p}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/Zhong-OWNS-R.pdf}
%         \caption{OWNS-R}
%         \label{fig:zhong-err-r}
%     \end{subfigure}\\
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/Zhong-OWNS-P-MM.pdf}
%         \caption{OWNS-P, MM only}
%         \label{fig:zhong-err-p-MM}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/Zhong-OWNS-R-MM.pdf}
%         \caption{OWNS-R, MM only}
%         \label{fig:zhong-err-r-MM}
%     \end{subfigure}
%     \caption{Convergence of the error for greedy and heuristic parameter selection for 2D high-speed boundary-layer flow.}
%     \label{fig:zhong-err}
% \end{figure}


\section{Demonstration of the greedy algorithm for spatial marching}\label{sec:greedyMarching}

Here we demonstrate that although the greedy algorithm must compute the eigenvalues of $M$, it leads to a net decrease in computational cost for linear and nonlinear disturbance evolution in low- and high-speed boundary-layer flows. In addition, we show that greedy selection alleviates numerical errors introduced by heuristic parameter selection for nonlinear disturbance evolution in a high-speed boundary-layer flow.


\subsection{3D low-speed oblique breakdown}

We first demonstrate for the  oblique breakdown case discussed in Section~\ref{sec:greedySingle} that greedy parameter selection reduces computational cost in both linear and nonlinear calculations. The problem parameters remain the same as before, and we consider the streamwise domain $Re_x \in [2.74, 6.08] \times 10^5$ with $N_x=2000$ stations. We compare the number of recursion parameters required for convergence of the relative error in the $N$-factor, which is a measure of the disturbance growth used in the $e^N$ method to empirically predict laminar-turbulent transition in boundary-layer flows~\cite{VanIngen_1956_eN,Smith_1956_eN}, and measure speed-up based on total wall-clock time. Note that the $N$-factor is defined as $N = \max_{x} \ln [ A(x) / A(x_0) ]$, where $A(x) = \max_{y} u'(x,y)$ is the disturbance amplitude and $x_0$ is domain inlet.


\subsubsection{Linear calculation}

OWNS-R with heuristic parameter selection does not remove left-going waves, so the march is unstable (see Section~\ref{subsec:stability}). Therefore, we compare OWNS-P and OWNS-R using greedy selection against OWNS-P using heuristic selection. To make meaningful comparisons, we take $N_\beta$ to be the smallest value that yields 10\%, 1\%, and 0.1\% relative error in the $N$-factor, measured against greedy OWNS-P with $N_\beta = 30$. Table~\ref{tab:oblique-lin-err} shows that greedy selection leads to a speed-up for both OWNS-R and OWNS-P, and that this speed-up pronounced for OWNS-R at 1\% error and OWNS-P and 0.1\% error.

\begin{table}
    \centering
    \begin{tabular}{ c | c c c | c c c }
              & & $N_\beta$ & & & Speed-up &\\
        Error & P-H & P-G & R-G & P-H & P-G & R-G\\ \hline
        10\%  & 11  & 5   & 29  & 1.0 & 2.3 & 3.6\\ \hline
        1\%   & 17  & 5   & 29  & 1.0 & 3.9 & 6.0\\ \hline
        0.1\% & 18  & 5   & 36  & 1.0 & 4.2 & 4.9
    \end{tabular}
    \caption{Speed-up achieved using greedy selection with OWNS-P (P-G) and OWNS-R (R-G) relative to heuristic selection with OWNS-P (P-H) for 3D low-speed oblique-wave breakdown.}
    \label{tab:oblique-lin-err}
\end{table}


\subsubsection{Nonlinear calculation}

For the nonlinear calculations, we truncate the Fourier series at $M=3$ temporal and $N=4$ spanwise modes, while we specify at the inlet that the oblique wave has amplitude 0.141\% based on the free-stream $u$-velocity. We select $N_\beta$ to be the smallest value such that the relative error in the $N$-factor is 0.1\% with respect to the reference solution (greedy selection with $N_\beta=30$). For NOWNS, heuristic and greedy parameter selection require $N_\beta = 18$ and $N_\beta = 6$, respectively, resulting in a speed-up of 3.83 for the greedy approach. 

% Figure~\ref{fig:oblique-nowns} compares solutions obtained using NOWNS with $N_\beta=6$ for greedy and heuristic selection, and shows that heuristic selection leads to solution blow-up, while greedy does not.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.7\linewidth]{figures/Oblique_NOWNS.pdf}
%     \caption{NOWNS for oblique-wave breakdown, comparing greedy and heuristic recursion parameter selection with $N_\beta=6$. Although NOWNS with greedy selection is sufficiently converged, heuristic selection is not, and the solution blows-up.}
%     \label{fig:oblique-nowns}
% \end{figure}

% \subsubsection{Linear calculation}

% We compute the $N$-factor based on the temperature, rather than velocity, disturbance and take the reference solution to be the greedy OWNS-P calculation with $N_\beta=20$. Table~\ref{tab:oblique-lin-MM-err} shows that heuristic recursion parameter selection generally converges faster than greedy, consistent with our observation in section~\ref{sec:zhong} that the error in Mack's second mode converges faster using heuristic selection. For linear calculations dominated by Mack's second mode, heuristic selection may be preferable. However, greedy selection may perform better for more complicated calculations involving more eigenvectors of $M$, which we demonstrate for our nonlinear calculation.

% \begin{table}
%     \centering
%     \begin{tabular}{ c | c c c | c c c }
%               & & $N_\beta$ & & & Speed-up &\\
%         Error & P-H & P-G & R-G & P-H & P-G & R-G\\ \hline
%         10\%  & 1  & 6   & 17  & 1.0 & 0.34 & 0.83\\ \hline
%         1\%   & 1  & 6   & 21  & 1.0 & 0.34 & 0.83\\ \hline
%         0.1\% & 2  & 7   & 22  & 1.0 & 0.32 & 0.82
%     \end{tabular}
%     \caption{Speed-up achieved by using greedy parameter selection with OWNS-P (P-G) and OWNS-R (R-G) relative to heuristic parameter selection with OWNS-P (P-H) for different relative errors in the $N$-factor for the 2D high-speed boundary-layer flow.}
%     \label{tab:oblique-lin-MM-err}
% \end{table}


% \subsubsection{Nonlinear calculation}

\subsection{2D high-speed Mack's second mode}

We apply NOWNS to the Mach 4.5 boundary-layer flow over an adiabatic flat plate studied by Ma and Zhong~\cite{Ma_2003_DNS_1}, where $M_\infty = 4.5$, $T_\infty^*=65.15$ K, $p_\infty^*=728.44$ Pa, $Pr=0.72$, and $Re_\infty^* = \rho_\infty^* U_\infty^* / \mu_\infty^*=7.2\times10^6 / $ m. For the disturbance frequency $F=2.2\times10^{-4}$, Mack's second mode (MM) is the dominant instability. We truncate the Fourier series at $M=10$ modes and specify that MM have an amplitude of 10\% based on the free-stream temperature. We compare the temperature profiles obtained using greedy and heuristic parameter selection in Figure~\ref{fig:NOWNS-Zhu-01}, and discuss how heuristic NOWNS introduces spurious numerical fluctuations along the sonic line.

Figure~\ref{fig:NOWNS-Zhu-T1-0500} shows that greedy and heuristic parameter selection predict similar temperature profiles for $T_1'$ near the inlet, while Figure~\ref{fig:NOWNS-Zhu-T1-2501} shows that at the domain outlet, heuristic NOWNS predicts a feature along the sonic line that is not predicted by greedy NOWNS. Figure~\ref{fig:NOWNS-Zhu-T3-0500} shows that near the domain inlet, heuristic NOWNS predicts a large fluctuation in $T_3'$ that is not predicted by greedy NOWNS. As the solution is marched downstream, this feature corrupts the solution, eventually leading to the fluctuation in $T_1'$ observed for heuristic NOWNS in Figure~\ref{fig:NOWNS-Zhu-T1-2501}. Although heuristic selection leads to accurate results for linear calculations for high-speed boundary-layer flows~\cite{Kamal_2020_HOWNS}, Figure~\ref{fig:NOWNS-Zhu-01} shows that the results become inaccurate for nonlinear calculations, and that greedy selection overcomes this challenge.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/NOWNS_Zhu_T1_0500_bw.pdf}
        \caption{$T_1'$ at $Re_x=6.92\times10^5$}\label{fig:NOWNS-Zhu-T1-0500}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/NOWNS_Zhu_T1_2501_bw.pdf}
        \caption{$T_1'$ at $Re_x=1.21\times10^6$}\label{fig:NOWNS-Zhu-T1-2501}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/NOWNS_Zhu_T3_0500_bw.pdf}
        \caption{$T_3'$ at $Re_x=6.92\times10^5$}\label{fig:NOWNS-Zhu-T3-0500}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/NOWNS_Zhu_T3_2501_bw.pdf}
        \caption{$T_3'$ at $Re_x=1.21\times10^6$}\label{fig:NOWNS-Zhu-T3-2501}
    \end{subfigure}    
    \caption{Temperature profiles for modes $T_1'$ and $T_3'$ computed for nonlinear evolution of Mack's second mode.}
    \label{fig:NOWNS-Zhu-01}
\end{figure}


% \begin{figure}
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/NOWNS_Zhu_Heuristic_T0.pdf}
%         \caption{Heuristic $T_0'$}\label{fig:NOWNS-Zhu-Heuristic-T0}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/NOWNS_Zhu_Heuristic_T1.pdf}
%         \caption{Heuristic $T_1'$}\label{fig:NOWNS-Zhu-Heuristic-T1}
%     \end{subfigure}
%     \caption{Temperature contours for $T_0'$ and $T_1'$ for nonlinear evolution of Mack's second mode with heuristic recursion parameter selection.}
%     \label{fig:NOWNS-Zhu-01}
% \end{figure}

% \begin{figure}
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/NOWNS_Zhu_Heuristic_T3.pdf}
%         \caption{Heuristic $T_3'$}\label{fig:NOWNS-Zhu-Heuristic-T3}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/NOWNS_Zhu_Greedy_T3.pdf}
%         \caption{Greedy $T_3'$}\label{fig:NOWNS-Zhu-Greedy-T3}
%     \end{subfigure}\\
%     \caption{Temperature contours for $T_3'$ for nonlinear evolution of Mack's second mode with heuristic and greedy recursion parameter selection. Note that the non-physical grid effects disappear when using greedy selection.}
%     \label{fig:NOWNS-Zhu-3}
% \end{figure}

% \subsection{Linear calculations for 2D low-speed and high-speed boundary-layer flows}

% For our 2D low-speed boundary-layer flow we demonstration in figure~\ref{fig:Bertolotti-Power} that updating the recursion parameters at each step of the march yields a relatively constant value of the objective function in $x$. We consider two approaches: (i) compute all eigenvalues of $M$ only at the inlet and update these recursion parameters at each subsequent station, and (ii) compute all eigenvalues of $M$ and perform the greedy algorithm again at regular intervals, while updating the recursion parameters for subsequent stations based on the closest left station. We see that both approaches yields small values of the objective function, but that the second approach (which entails a higher computational cost) yields smaller values of the objective function. Figure~\ref{fig:Pruett-Power} demonstrates the interpolation procedure for the Mach 4.5 boundary-layer flow. The number of left- and right-going characteristics changes 23 times between $Re_x=6.4\times10^5$ and $Re_x=1.44\times10^6$, so that we must compute all the eigenvalues of $M$ 23 times, which increases the cost of the greedy approach relative to subsonic flows.

% \begin{figure}
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/Bertolotti_Power_Update.pdf}
%         \caption{2D low-speed boundary-layer flow with $N_\beta=20$.}
%         \label{fig:Bertolotti-Power}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/Pruett_Power_2.pdf}
%         \caption{2D high-speed boundary-layer flow with $N_\beta=15$.}
%         \label{fig:Pruett-Power}
%     \end{subfigure}
%     \caption{Objective function as a function of streamwise station using MATLAB \texttt{eigs} to update recursion parameters.}
% \end{figure}


\section{Conclusion}


Through numerical analyses and experiments, we have compared the convergence properties of the recursive filters used by OWNS-P and OWNS-R, and introduced a greedy algorithm to automatically choose recursion parameters that ensure rapid convergence of the filter error. Although we have used the OWNS label, which refers to the method when generalized to the linearized Navier-Stokes equations, we would like to emphasize that all of the analyses also apply to systems of linear first-order hyperbolic equation, and that the greedy algorithm could also be applied to such systems. In summary, we showed that OWNS-P has superior error convergence properties, so that there exist recursion parameter sets for which OWNS-P is fully converged, while OWNS-R is not. Conversely, if OWNS-R is fully converged, then OWNS-P must also be fully converged. We found for boundary-layer flows that the heuristic parameter selection that yields a stable and accurate OWNS-P march is inaccurate and unstable for OWNS-R. The underlying issue appears to be the rounding errors associated with finite precision arithmetic introduced when factoring the polynomial~\eqref{eq:owns-r-polynomial} and with solving the system~\eqref{eq:filter-r}. Therefore, future work should investigate how to modify the OWNS-R routine to minimize the impact of this rounding error, for example, using higher precision arithmetic.

We demonstrated for linear and nonlinear disturbance evolution in boundary-layer flows that our proposed greedy algorithm yields faster error convergence and a reduced computational cost relative to heuristic parameter selection. We further demonstrated the heuristic NOWNS for high-speed boundary-layer flows introduces spurious numerical oscillations, which are eliminated using the greedy algorithm, suggesting that the existing heuristic routine is not sufficiently accurate for nonlinear calculations. We compared the greedy and heuristic recursion parameter sets, and highlighted that greedy OWNS converges more quickly since it places its eigenvalues more efficiently, and we note that future work could use greedy parameter selection to help tune the heuristic routines. We further emphasize that the heuristic parameter selection routine requires an analytical approximation to the eigenvalues of the governing equations (e.g., the eigenvalues of the Euler equations linearized about a uniform flow can be computed analytically) and a method to use this approximation to construct sets of convergence recursion parameters. For equations of interest, an analytical approximation to the eigenvalues may not exist, and even if it does, it may not be obvious how to use the expression to construct convergent recursion parameters sets. The greedy algorithm allows for automatic recursion parameter selection without the need for analytical approximations, which will allow the OWNS approach to be applied more easily to new systems of linear first-order hyperbolic equations. Finally, we note that this work focused only on flows with a single inhomogeneous direction (the wall-normal direction), and we note that the approach may not extend readily to flows with two or more inhomogeneous directions due to the increased grid size, and so future work should explore how to adapt the greedy algorithm for such cases.


\section*{Acknowledgments}

This work has been supported by The Boeing Company through the Strategic Research and Development Relationship
Agreement CT-BA-GTA-1.

\appendix
\input{appProofs}
\input{appRounding}

\bibliographystyle{siamplain}
\bibliography{ownslibrary}
\end{document}
